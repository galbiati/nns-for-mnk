{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import lasagne\n",
    "import loading\n",
    "from training import *\n",
    "from network import *\n",
    "from architectures import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import bayes_mvs as bmvs\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# aliases\n",
    "L = lasagne.layers\n",
    "nl = lasagne.nonlinearities\n",
    "T = theano.tensor\n",
    "\n",
    "# data loading\n",
    "headdir = os.path.expanduser('~/Google Drive/Bas Zahy Gianni - Games')\n",
    "paramsdir = os.path.join(headdir, 'Analysis/0_hvh/Params/nnets/temp')\n",
    "datafile = os.path.join(headdir, 'Data/0_hvh/Clean/_summaries/model_input_with_groups.csv')\n",
    "resultsdir = os.path.join(headdir, 'Analysis/0_hvh/Loglik/nnets')\n",
    "data = loading.default_loader(datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prototype = default_convnet\n",
    "\n",
    "def prototype(input_var=None):\n",
    "    input_shape = (None, 2, 4, 9)\n",
    "    FixLayer = make_FixLayer(input_var)\n",
    "\n",
    "    input_layer = L.InputLayer(shape=input_shape, input_var=input_var)\n",
    "    network = L.Conv2DLayer(\n",
    "        input_layer, num_filters=32, filter_size=(4,4), pad='full',\n",
    "        nonlinearity=nl.identity\n",
    "    )\n",
    "\n",
    "    network = L.ParametricRectifierLayer(network, shared_axes='auto') # default: auto\n",
    "    network = L.FeaturePoolLayer(network, pool_function=T.sum, pool_size=2) # default: T.sum, 2\n",
    "    network = L.DropoutLayer(network, p=.75) # default: .75\n",
    "    network = L.DenseLayer(\n",
    "        network, num_units=36,\n",
    "        nonlinearity=nl.very_leaky_rectify, W=lasagne.init.HeUniform(gain='relu')\n",
    "    )\n",
    "    network = FixLayer(network)\n",
    "    network = L.NonlinearityLayer(network, nonlinearity=nl.softmax)\n",
    "    network = FixLayer(network)\n",
    "    network = ReNormLayer(network)\n",
    "\n",
    "    return network\n",
    "\n",
    "trainer = DefaultTrainer(stopthresh=100) # default: 125\n",
    "net_list = trainer.train_all(architecture=prototype, data=data, seed=985227)\n",
    "\n",
    "for i, n in enumerate(net_list):\n",
    "    n.save_params(os.path.join(paramsdir, '{} split agg fit'.format(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject finetuning prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prototype(input_var=None):\n",
    "    input_shape = (None, 2, 4, 9)\n",
    "    FixLayer = make_FixLayer(input_var)\n",
    "\n",
    "    input_layer = L.InputLayer(shape=input_shape, input_var=input_var)\n",
    "    network = L.Conv2DLayer(\n",
    "        input_layer, num_filters=32, filter_size=(4,4), pad='full',\n",
    "        nonlinearity=nl.identity\n",
    "    )\n",
    "\n",
    "    network = L.ParametricRectifierLayer(network, shared_axes='auto') # default: auto\n",
    "    network = L.FeaturePoolLayer(network, pool_function=T.sum, pool_size=4) # default: T.sum, 2\n",
    "    network = L.DropoutLayer(network, p=.875) # default: .75\n",
    "    network = L.DenseLayer(\n",
    "        network, num_units=36,\n",
    "        nonlinearity=nl.leaky_rectify, W=lasagne.init.HeUniform(gain='relu')\n",
    "    )\n",
    "    network = FixLayer(network)\n",
    "    network = L.NonlinearityLayer(network, nonlinearity=nl.softmax)\n",
    "    network = FixLayer(network)\n",
    "    network = ReNormLayer(network)\n",
    "\n",
    "    return network\n",
    "\n",
    "class FineTuner(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    Trainer to fine tune networks to individual subjects\n",
    "\n",
    "    Consider moving freeze, param set functions properly into Network object\n",
    "    Abstracting split functions and augment in DefaultTrainer would be good too\n",
    "    \"\"\"\n",
    "\n",
    "    def train_all(self, architecture, data, split=0, seed=None, startparams=None, freeze=True, save_params=False):\n",
    "        if seed:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        D, groups, Xs, ys, Ss = data\n",
    "        num_splits = len(Xs)\n",
    "        r = np.tile(np.arange(num_splits), [num_splits, 1])\n",
    "        r = (r + r.T) % num_splits\n",
    "\n",
    "        starttime = time.time()\n",
    "        net = Network(architecture)\n",
    "        if startparams:\n",
    "            _layers = L.get_all_layers(net.net)[1:3]\n",
    "            L.set_all_param_values(net.net, startparams)#(_layers, startparams[0:3])\n",
    "            convlayer, prelulayer = _layers\n",
    "            if freeze:\n",
    "                convlayer.params[convlayer.W].remove('trainable')\n",
    "                convlayer.params[convlayer.b].remove('trainable')\n",
    "                prelulayer.params[prelulayer.alpha].remove('trainable')\n",
    "\n",
    "        train_idxs = r[split, :3]\n",
    "        val_idxs = r[split, 3:4]\n",
    "        test_idxs = r[split, 4:]\n",
    "\n",
    "        X, y, S = [np.concatenate(np.array(Z)[train_idxs]) for Z in [Xs, ys, Ss]]\n",
    "        Xv, yv, Sv = [np.concatenate(np.array(Z)[val_idxs]) for Z in [Xs, ys, Ss]]\n",
    "        Xt, yt, St = [np.concatenate(np.array(Z)[test_idxs]) for Z in [Xs, ys, Ss]]\n",
    "#         print(X.shape[0])\n",
    "        if X.shape[0] > 75:\n",
    "            X, y = augment((X, y))\n",
    "            S = np.concatenate([S, S, S, S])\n",
    "            self.train(net, training_data=(X, y), validation_data=(Xv, yv))\n",
    "        self.test(net, testing_data=(Xt, yt))\n",
    "        time_elapsed = time.time() - starttime\n",
    "\n",
    "        return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: save traces\n",
    "# TODO: keep traces from one net to another?? or save separately and append...\n",
    "\n",
    "# set fixed params with easy-access dicts, because we don't hate ourselves\n",
    "#   also because we don't hate ourselves, maybe name these more clearly soon?\n",
    "\n",
    "trainer_settings = {\n",
    "    'stopthresh':3,\n",
    "#     'updates': lasagne.updates.sgd,\n",
    "    'update_args':{'learning_rate':.001}\n",
    "}\n",
    "\n",
    "trainer_args = {\n",
    "    'architecture':prototype,\n",
    "    'seed':985227,\n",
    "}\n",
    "\n",
    "subject_Xs = data[2]\n",
    "subject_ys = data[3]\n",
    "subject_Ss = data[4]\n",
    "results = []\n",
    "for i, net in enumerate(net_list):\n",
    "    result = []\n",
    "    PARAMS = L.get_all_param_values(net.net)\n",
    "    \n",
    "    for subno in range(40):\n",
    "        S0 = [np.where(s==subno)[0] for s in subject_Ss]\n",
    "        X0 = [subject_Xs[i][s] for i, s in enumerate(S0)]\n",
    "        y0 = [subject_ys[i][s] for i, s in enumerate(S0)]\n",
    "        sub_data = (data[0], data[1], X0, y0, S0)\n",
    "        print('\\nMODEL {}, SUBJECT {}, NMOVES {} per split\\n'.format(i, subno, len(S0[0])))\n",
    "\n",
    "        batchsize = len(S0[0])//2\n",
    "        sub_trainer = FineTuner(batchsize=batchsize, **trainer_settings)\n",
    "        sub_net = sub_trainer.train_all(data=sub_data, startparams=PARAMS, split=i, **trainer_args)\n",
    "        result.append(sub_net.test_err)\n",
    "        sub_net.save_params(os.path.join(paramsdir, '{} split {} sub tune fit'.format(i, subno))) # something is broken here\n",
    "        # add trace saving here\n",
    "        \n",
    "        \n",
    "    results.append(result)\n",
    "\n",
    "res = np.array(results).T\n",
    "# np.savetxt(os.path.join(resultsdir, 'nlls.txt'), res, fmt='%.18f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate boards file\n",
    "\n",
    "# TEMPORARY subclass - merge these functions into main network, since they are useful!\n",
    "class TestNetwork(Network):\n",
    "    def __init__(self, architecture):\n",
    "        super(TestNetwork, self).__init__(architecture)\n",
    "        self.itemized_loss = lasagne.objectives.categorical_crossentropy(self.test_prediction, self.target_var)\n",
    "        self.itemized_test_fn = theano.function([self.input_var, self.target_var], self.itemized_loss)\n",
    "        \n",
    "    def load_params(self, paramsfile):\n",
    "        with np.load(paramsfile) as loaded:\n",
    "            params_list = [(i[0], i[1]) for i in loaded.items()]\n",
    "            params_list.sort()\n",
    "            L.set_all_param_values(self.net, [i[1] for i in params_list])\n",
    "            \n",
    "\n",
    "D = data[0].copy()\n",
    "D2 = data[0].copy()\n",
    "D['group'] = D['group']-1\n",
    "D2['group'] = D2['group']-1\n",
    "O = pd.DataFrame(index=D.index, columns=list(range(36)))\n",
    "Xx, yy, Ss, Gg, Npnp = loading.unpack_data(D)\n",
    "for _g in D['group'].unique():\n",
    "    print('Split group: {}, Model idx: {}\\n'.format(_g, (_g+1)%5))\n",
    "    for _s in D['subject'].unique():\n",
    "        paramsfile = os.path.join(paramsdir, '{} split {} sub tune fit.npz'.format((_g+1)%5, _s))\n",
    "        locs = (D['subject']==_s)&(D['group']==_g)\n",
    "        idx = D.loc[locs, :].index.values\n",
    "        test_net = TestNetwork(prototype)\n",
    "        test_net.load_params(paramsfile)\n",
    "        \n",
    "        _x = Xx[idx, :, :, :]\n",
    "        _y = yy[idx]\n",
    "        print(_s, test_net.test_fn(_x, _y))\n",
    "\n",
    "        D.loc[idx, 'nll'] = test_net.itemized_test_fn(_x, _y)\n",
    "        O.loc[idx, list(range(36))] = test_net.output_fn(_x)\n",
    "        O.loc[idx, 'prediction'] = O.loc[idx, list(range(36))].values.argmax(axis=1)\n",
    "    paramsfile = os.path.join(paramsdir, '{} split agg fit.npz'.format((_g+1)%5))\n",
    "    idx = D2.loc[D2['group']==_g, :].index.values\n",
    "    test_net = TestNetwork(prototype)\n",
    "    test_net.load_params(paramsfile)\n",
    "    \n",
    "    _x = Xx[idx, :, :, :]\n",
    "    _y = yy[idx]\n",
    "    D2.loc[idx, 'nll'] = test_net.itemized_test_fn(_x, _y)\n",
    "\n",
    "D = D.join(O)\n",
    "D['correct'] = (D['zet']==D['prediction']).astype(int)\n",
    "D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = D.pivot_table(index='subject', values=['nll']).values.flatten()\n",
    "p2 = D2.pivot_table(index='subject', values=['nll']).values.flatten()\n",
    "diff = p - p2\n",
    "lendiff = D.pivot_table(index='subject', values=['nll'], aggfunc=len).values.flatten()\n",
    "order = p2.argsort()\n",
    "from scipy.stats import linregress\n",
    "lr = linregress(lendiff, diff)\n",
    "print(lr)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,8))\n",
    "ax = axes[0, 0]\n",
    "ax.plot(lendiff, diff, color='black', linestyle='none', marker='o')\n",
    "ax.text(25, -.18, s='corr = {:.5f}'.format(lr.slope))\n",
    "plt.setp(axes[0, 0], xlabel='N Moves in data', ylabel='Delta NLL')\n",
    "ax = axes[0, 1]\n",
    "ax.plot(lendiff, p2, color='black', linestyle='none', marker='o')\n",
    "plt.setp(ax, xlabel='N Moves in data', ylabel='NLL')\n",
    "ax = axes[1, 1]\n",
    "ax.plot(lendiff, p, color='red', linestyle='none', marker='o')\n",
    "plt.setp(ax, xlabel='N Moves in data', ylabel='NLL')\n",
    "\n",
    "axes[1, 0].plot(np.arange(40), p2[order], color='black', linestyle='none', marker='o', label='No tune')\n",
    "axes[1, 0].plot(np.arange(40), p[order], color='red', linestyle='none', marker='o', label='Fine tune')\n",
    "axes[1, 0].legend(loc=0)\n",
    "plt.setp(axes[1, 0], xlabel='Ranked subject', ylabel='Nll')\n",
    "sns.despine()\n",
    "fig.savefig(os.path.join(resultsdir, 'overfitting viz exclude n lt 125.png'), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D2['nll'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D.to_csv(os.path.join(resultsdir, '(POSSIBLE BUG - NLL MISMATCH) appended data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mos = [bmvs(res[i, :], alpha=.95) for i in range(res.shape[0])]\n",
    "means = np.array([m[0][0] for m in mos])\n",
    "lbs = np.array([m[0][1][0] for m in mos])\n",
    "ubs = np.array([m[0][1][1] for m in mos])\n",
    "idx = np.argsort(means)\n",
    "plt.fill_between(x=np.arange(40), y1=lbs[idx], y2=ubs[idx], alpha=.2)\n",
    "plt.plot(means[idx])\n",
    "plt.setp(plt.gca(), xlabel='Ranked Subject', ylabel='NLL')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(res.flatten(), bins = np.arange(1, 4, 1/8))\n",
    "plt.setp(plt.gca(), xlabel='NLL', ylabel='# Fits')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bmvs(np.array(res).flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def autoencoder_prototype(input_var=None):\n",
    "    input_shape = (None, 2, 4, 9)\n",
    "    inpuL.InputLayer = L.InputLayer(shape=input_shape, input_var=input_var)\n",
    "    network = L.COnv2DLayer(\n",
    "        input_layer, num_filters=32, filter_size=(4, 4), pad='full',\n",
    "        nonlinearity=nl.identity\n",
    "    )\n",
    "    L.ParametricRectifierLayerrLayeshared_axes=Rectifier(network, shL.FeaturePoolLayerl')\n",
    "    network = L.FeaturePoolLayer(network, pool_functioL.DropoutLayerm, pool_size=2)\n",
    "    network = L.DropoutLayer(network, p=.75)\n",
    "    network = L.DenseLayer(\n",
    "        network, num_units=72,\n",
    "        nonlinearity=nl.very_leaky_rectify, W=lasagne.init.HeUniform(gain='relu')\n",
    "    )\n",
    "    network = L.ReshapeLayer(\n",
    "        network, shape=(-1, 2, 4, 9)\n",
    "    )\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Autoencoder(Network):\n",
    "    \"\"\"\n",
    "    Overwrites loss functions for training an autoencoder instead\n",
    "    of softmax output classifier\n",
    "    \"\"\"\n",
    "    def objectives(self):\n",
    "        \"\"\"Must define: loss, test_loss, test_acc\"\"\"\n",
    "        self.loss = lasagne.objectives.squared_error(self.prediction, self.target_var)\n",
    "        self.loss = self.loss.mean()\n",
    "        self.test_loss = lasagne.objectives.squared_error(self.test_prediction, self.target_var)\n",
    "        self.test_loss = self.test_loss.mean()\n",
    "        self.test_acc = T.mean(T.abs(self.test_prediction - self.target_var))\n",
    "        return None\n",
    "\n",
    "# class AutoTrainer(DefaultTrainer):\n",
    "#     \"\"\"\n",
    "#     Overwrites training functions for training an autoencoder instead\n",
    "#     of softmax output classifier\n",
    "#     \"\"\"\n",
    "#     pass\n",
    "\n",
    "# Can actually pass data to existing trainer by replacing ys with a copy of Xs!\n",
    "\n",
    "# class LadderNet(Network):\n",
    "#     \"\"\"\n",
    "#     Loads conv layer params from autoencoder save and sets them on current classifier\n",
    "#     Needs overwrite of self.build() that loads conv2d params, then sets them untrainable.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, architecture, paramsfile):\n",
    "#         self.paramsfile = paramsfile\n",
    "#         super(LadderNet, self).__init__(architecture)\n",
    "    \n",
    "#     def build(self):\n",
    "#         super(LadderNet, self).build()\n",
    "        \n",
    "#         # Finish this soon!\n",
    "#         return None\n",
    "    \n",
    "#     def load_conv_params(self):\n",
    "#         with np.load(paramsfile) as loaded:\n",
    "# #             L."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
