{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import lasagne\n",
    "import loading\n",
    "from training import *\n",
    "from network import *\n",
    "from architectures import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import bayes_mvs as bmvs\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('poster')\n",
    "colors = sns.color_palette()\n",
    "\n",
    "# aliases\n",
    "L = lasagne.layers\n",
    "nl = lasagne.nonlinearities\n",
    "T = theano.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "headdir = os.path.expanduser('~/Google Drive/Bas Zahy Gianni - Games')\n",
    "paramsdir = os.path.join(headdir, 'Analysis/0_hvh/Params/nnets/temp')\n",
    "datadir = os.path.join(headdir, 'Data/model input')\n",
    "resultsdir = os.path.join(headdir, 'Analysis/0_hvh/Loglik/nnets')\n",
    "datafilenames = ['0 (with groups)', '1 (with computer)', '2 (with computer)', '3 (with computer)', '4']\n",
    "datafilenames = [os.path.join(datadir, fname + '.csv') for fname in datafilenames]\n",
    "colnames = ['subject', 'color', 'bp', 'wp', 'zet', 'rt']\n",
    "\n",
    "e0 = pd.read_csv(datafilenames[0], names=colnames+['splitno'])\n",
    "e1 = pd.read_csv(datafilenames[1], names=colnames)\n",
    "e2 = pd.read_csv(datafilenames[2], names=colnames)\n",
    "e3 = pd.read_csv(datafilenames[3], names=colnames+['task', 'taskorder', 'session'])\n",
    "e4 = pd.read_csv(datafilenames[4], names=colnames+['timecondition'])\n",
    "Es = [e1, e2, e3, e4]\n",
    "for i, e in enumerate(Es[1:]):\n",
    "    e['subject'] = e['subject'] + Es[i-1].loc[Es[i-1]['subject']<1000, 'subject'].max()\n",
    "\n",
    "A = pd.concat([e[colnames] for e in [e1, e2, e3, e4]])\n",
    "\n",
    "groups = np.arange(len(A))%5 + 1\n",
    "np.random.seed(100001)\n",
    "np.random.shuffle(groups)\n",
    "A['group'] = groups\n",
    "\n",
    "A.to_csv(os.path.join(datadir, '1-4.csv'), encoding='ASCII', header=False, index=False)\n",
    "A.loc[A['subject']<1000, :].to_csv(\n",
    "    os.path.join(datadir, '1-4 (no computer).csv'), \n",
    "    encoding='ASCII', header=False, index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class FineTuner(DefaultTrainer):\n",
    "#     \"\"\"\n",
    "#     Trainer to fine tune networks to individual subjects\n",
    "\n",
    "#     Consider moving freeze, param set functions properly into Network object\n",
    "#     Abstracting split functions and augment in DefaultTrainer would be good too\n",
    "#     \"\"\"\n",
    "\n",
    "#     def train_all(self, architecture, data, split, seed=None, startparams=None, freeze=True, save_params=False):\n",
    "#         if seed:\n",
    "#             np.random.seed(seed)\n",
    "\n",
    "#         D, groups, Xs, ys, Ss = data\n",
    "#         num_splits = len(Xs)\n",
    "#         r = np.tile(np.arange(num_splits), [num_splits, 1])\n",
    "#         r = (r + r.T) % num_splits\n",
    "\n",
    "#         starttime = time.time()\n",
    "#         net = Network(architecture)\n",
    "#         if startparams:\n",
    "#             _layers = L.get_all_layers(net.net)\n",
    "#             L.set_all_param_values(_layers, startparams)\n",
    "#             convlayer, prelulayer = _layers[1:3]\n",
    "#             if freeze:\n",
    "#                 convlayer.params[convlayer.W].remove('trainable')\n",
    "#                 convlayer.params[convlayer.b].remove('trainable')\n",
    "#                 prelulayer.params[prelulayer.alpha].remove('trainable')\n",
    "\n",
    "#         train_idxs = r[split, :3]\n",
    "#         val_idxs = r[split, 3:4]\n",
    "#         test_idxs = r[split, 4:]\n",
    "\n",
    "#         X, y, S = [np.concatenate(np.array(Z)[train_idxs]) for Z in [Xs, ys, Ss]]\n",
    "#         Xv, yv, Sv = [np.concatenate(np.array(Z)[val_idxs]) for Z in [Xs, ys, Ss]]\n",
    "#         Xt, yt, St = [np.concatenate(np.array(Z)[test_idxs]) for Z in [Xs, ys, Ss]]\n",
    "#         X, y = augment((X, y))\n",
    "#         S = np.concatenate([S, S, S, S])\n",
    "#         self.train(net, training_data=(X, y), validation_data=(Xv, yv))\n",
    "#         self.test(net, testing_data=(Xt, yt))\n",
    "#         time_elapsed = time.time() - starttime\n",
    "\n",
    "#         return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = loading.default_loader(os.path.join(datadir, '1-4 (no computer).csv'))\n",
    "hvhdata = loading.default_loader(os.path.join(datadir, '0 (with groups).csv'))\n",
    "Xs = np.concatenate(hvhdata[2])\n",
    "ys = np.concatenate(hvhdata[3])\n",
    "Ss = np.concatenate(hvhdata[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping\n",
    "\n",
    "REMEMBER: you need to replace `prototype` with `archX`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_full_fit(num_filters, pool_size=2, filter_size=(4,4)):\n",
    "    \"\"\"\n",
    "    Runs the full fitting experiment, pretraining on later experiments and testing on first.\n",
    "    Saves data as it goes to avoid eating memory.\n",
    "    \"\"\"\n",
    "    # define architecture\n",
    "    archname = 'arch{}'.format(num_filters)\n",
    "    arch = lambda input_var: prototype(input_var, num_filters=num_filters, pool_size=pool_size, filter_size=filter_size)\n",
    "    \n",
    "    # start training\n",
    "    trainer = DefaultTrainer(stopthresh=100)\n",
    "    net_list = trainer.train_all(architecture=arch, data=data, seed=985227)\n",
    "    \n",
    "    # save params\n",
    "    for i, n in enumerate(net_list):\n",
    "        fname = '{} {} split agg fit exp 1-4'.format(archname, i)\n",
    "        n.save_params(os.path.join(paramsdir, fname))\n",
    "    \n",
    "    # save high board-by-board nlls\n",
    "        # WARNING: does not preserve board order!!!\n",
    "    nlls, results = [], []\n",
    "    for net in net_list:\n",
    "        nll = net.test_fn(Xs, ys)\n",
    "        nlls.append(nll[0])\n",
    "        res = net.itemized_test_fn(Xs, ys)\n",
    "        results.append(res)\n",
    "\n",
    "    nlls_array = np.array(nlls)\n",
    "    results_array = np.array(results)\n",
    "    print(\"\\nPretrain stats\\n\", bmvs(nlls_array))\n",
    "    \n",
    "    # save results per subject\n",
    "    subresults = []\n",
    "    for s in np.unique(Ss):\n",
    "        idx = np.where(Ss==s)[0]\n",
    "        res = results_array[:, idx].mean()\n",
    "        subresults.append(res)\n",
    "        \n",
    "    subresults_array = np.array(subresults)\n",
    "    fname = '{} agg fit exp 1-4 results by subject.csv'.format(archname)\n",
    "    fname = os.path.join(resultsdir, fname)\n",
    "    np.savetxt(fname, subresults_array, fmt='%.18f', delimiter=',')\n",
    "        \n",
    "    print(\"\\nBy subject\\n\", bmvs(subresults_array, alpha=.95))\n",
    "    \n",
    "    # fine tune on hvh data\n",
    "    \n",
    "    tune_results = np.zeros([40, 5, 5])\n",
    "    tuner = FineTuner(stopthresh=25)\n",
    "\n",
    "    for i, n in enumerate(net_list):\n",
    "        for j in range(5):\n",
    "            fname = '{} {} agg fit exp 1-4 {} tune fit exp 0'.format(archname, i, j)\n",
    "            params = L.get_all_param_values(n.net)\n",
    "            subnet = tuner.train_all(architecture=arch, data=hvhdata, split=j, startparams=params, freeze=True)\n",
    "            subnet.save_params(os.path.join(paramsdir, fname))\n",
    "            res = subnet.itemized_test_fn(Xs, ys)\n",
    "            \n",
    "            for s in np.unique(Ss):\n",
    "                idx = np.where(Ss==s)[0]\n",
    "                tune_results[s, j, i] = res[idx].mean()\n",
    "                \n",
    "    fname = '{} agg fit exp 1-4 agg tune fit exp 0.csv'.format(archname)\n",
    "    fname = os.path.join(resultsdir, fname)\n",
    "    np.savetxt(fname, tune_results.reshape([40, 25]), fmt='%.18f', delimiter=',')\n",
    "                \n",
    "    print(\"\\nFine tune result by subject\\n\", bmvs(tune_results.mean(axis=(1,2))))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split Number 0\n",
      "(12362, 2, 4, 9)\n",
      "Epoch 0 took 3.693s\n",
      "\ttraining loss:\t\t\t2.8708\n",
      "\tvalidation loss:\t\t2.7153\n",
      "\tvalidation accuracy:\t\t16.73%\n",
      "\ttotal time elapsed:\t\t3.789s\n",
      "Epoch 50 took 3.407s\n",
      "\ttraining loss:\t\t\t2.5360\n",
      "\tvalidation loss:\t\t2.3982\n",
      "\tvalidation accuracy:\t\t27.36%\n",
      "\ttotal time elapsed:\t\t169.918s\n",
      "Epoch 100 took 3.233s\n",
      "\ttraining loss:\t\t\t2.5315\n",
      "\tvalidation loss:\t\t2.3969\n",
      "\tvalidation accuracy:\t\t27.49%\n",
      "\ttotal time elapsed:\t\t335.583s\n",
      "Epoch 150 took 2.964s\n",
      "\ttraining loss:\t\t\t2.5317\n",
      "\tvalidation loss:\t\t2.3876\n",
      "\tvalidation accuracy:\t\t27.34%\n",
      "\ttotal time elapsed:\t\t497.594s\n",
      "Abandon ship!\n",
      "\n",
      "TEST PERFORMANCE\n",
      "\tStopped in epoch:\t\t165\n",
      "\tTest loss:\t\t\t2.3963\n",
      "\tTest accuracy:\t\t\t27.61%\n",
      "\n",
      "Split Number 1\n",
      "(12396, 2, 4, 9)\n",
      "Epoch 0 took 3.269s\n",
      "\ttraining loss:\t\t\t2.8940\n",
      "\tvalidation loss:\t\t2.7624\n",
      "\tvalidation accuracy:\t\t16.16%\n",
      "\ttotal time elapsed:\t\t3.362s\n",
      "Epoch 50 took 3.040s\n",
      "\ttraining loss:\t\t\t2.5418\n",
      "\tvalidation loss:\t\t2.4054\n",
      "\tvalidation accuracy:\t\t27.43%\n",
      "\ttotal time elapsed:\t\t173.792s\n",
      "Epoch 100 took 3.027s\n",
      "\ttraining loss:\t\t\t2.5358\n",
      "\tvalidation loss:\t\t2.4031\n",
      "\tvalidation accuracy:\t\t28.01%\n",
      "\ttotal time elapsed:\t\t330.145s\n",
      "Epoch 150 took 3.617s\n",
      "\ttraining loss:\t\t\t2.5306\n",
      "\tvalidation loss:\t\t2.3944\n",
      "\tvalidation accuracy:\t\t28.19%\n",
      "\ttotal time elapsed:\t\t486.940s\n",
      "Abandon ship!\n",
      "\n",
      "TEST PERFORMANCE\n",
      "\tStopped in epoch:\t\t180\n",
      "\tTest loss:\t\t\t2.3949\n",
      "\tTest accuracy:\t\t\t28.43%\n",
      "\n",
      "Split Number 2\n",
      "(12511, 2, 4, 9)\n",
      "Epoch 0 took 3.462s\n",
      "\ttraining loss:\t\t\t2.8971\n",
      "\tvalidation loss:\t\t2.7353\n",
      "\tvalidation accuracy:\t\t16.72%\n",
      "\ttotal time elapsed:\t\t3.555s\n",
      "Epoch 50 took 2.999s\n",
      "\ttraining loss:\t\t\t2.5637\n",
      "\tvalidation loss:\t\t2.4280\n",
      "\tvalidation accuracy:\t\t27.29%\n",
      "\ttotal time elapsed:\t\t168.591s\n",
      "Epoch 100 took 3.057s\n",
      "\ttraining loss:\t\t\t2.5546\n",
      "\tvalidation loss:\t\t2.4086\n",
      "\tvalidation accuracy:\t\t27.82%\n",
      "\ttotal time elapsed:\t\t327.042s\n",
      "Epoch 150 took 3.120s\n",
      "\ttraining loss:\t\t\t2.5528\n",
      "\tvalidation loss:\t\t2.4131\n",
      "\tvalidation accuracy:\t\t28.02%\n",
      "\ttotal time elapsed:\t\t490.212s\n"
     ]
    }
   ],
   "source": [
    "run_full_fit(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping Tuning\n",
    "\n",
    "### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = DefaultTrainer(stopthresh=100) # default: 125\n",
    "net_list = trainer.train_all(architecture=prototype, data=data, seed=985227)\n",
    "\n",
    "for i, n in enumerate(net_list):\n",
    "    n.save_params(os.path.join(paramsdir, '{} split agg fit exp 1-4'.format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subresults = []\n",
    "for s in np.unique(Ss):\n",
    "    idx = np.where(Ss==s)[0]\n",
    "    res = results_array[:, idx].mean()\n",
    "    subresults.append(res)\n",
    "bmvs(np.array(subresults), alpha=.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tune_results = np.zeros([40, 5, 5])\n",
    "tuner = FineTuner(stopthresh=25)\n",
    "\n",
    "for i, n in enumerate(net_list):\n",
    "    for j in range(5):\n",
    "        params = L.get_all_param_values(n.net)\n",
    "        subnet = tuner.train_all(architecture=prototype, data=hvhdata, split=j, startparams=params, freeze=True)\n",
    "        subnet.save_params(os.path.join(paramsdir, '{} agg fit exp 1-4 {} tune fit exp 0'.format(i, j)))\n",
    "        \n",
    "        res = subnet.itemized_test_fn(Xs, ys)\n",
    "        for s in np.unique(Ss):\n",
    "            idx = np.where(Ss==s)[0]\n",
    "            tune_results[s, j, i] = res[idx].mean()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
