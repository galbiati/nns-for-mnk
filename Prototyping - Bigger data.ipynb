{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import lasagne\n",
    "import loading\n",
    "from training import *\n",
    "from network import *\n",
    "from architectures import *\n",
    "\n",
    "# aliases\n",
    "L = lasagne.layers\n",
    "nl = lasagne.nonlinearities\n",
    "T = theano.tensor\n",
    "\n",
    "# directories\n",
    "headdir = os.path.expanduser('~/Google Drive/Bas Zahy Gianni - Games')\n",
    "paramsdir = os.path.join(headdir, 'Analysis/0_hvh/Params/nnets/temp')\n",
    "datadir = os.path.join(headdir, 'Data/model input')\n",
    "resultsdir = os.path.join(headdir, 'Analysis/0_hvh/Loglik/nnets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = loading.default_loader(os.path.join(datadir, '1-4 (no computer).csv'))\n",
    "hvhdata = loading.default_loader(os.path.join(datadir, '0 (with groups).csv'))\n",
    "Xs = np.concatenate(hvhdata[2])\n",
    "ys = np.concatenate(hvhdata[3])\n",
    "Ss = np.concatenate(hvhdata[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_full_fit(arch, archname):\n",
    "    \"\"\"\n",
    "    Runs the full fitting experiment, pretraining on later experiments and testing on first.\n",
    "    Saves data as it goes to avoid eating memory.\n",
    "    \"\"\"\n",
    "\n",
    "    # start training\n",
    "    trainer = DefaultTrainer(stopthresh=75, print_interval=20)\n",
    "    net_list = trainer.train_all(architecture=arch, data=data, seed=985227)\n",
    "    \n",
    "    # save params\n",
    "    for i, n in enumerate(net_list):\n",
    "        fname = '{} {} split agg fit exp 1-4'.format(archname, i)\n",
    "        n.save_params(os.path.join(paramsdir, fname))\n",
    "    \n",
    "    tuner = FineTuner(stopthresh=20)\n",
    "\n",
    "    for i, n in enumerate(net_list):\n",
    "        for j in range(5):\n",
    "            fname = '{} {} agg fit exp 1-4 {} tune fit exp 0'.format(archname, i, j)\n",
    "            params = L.get_all_param_values(n.net)\n",
    "            subnet = tuner.train_all(architecture=arch, data=hvhdata, split=j, startparams=params, freeze=True)\n",
    "            subnet.save_params(os.path.join(paramsdir, fname))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "archkws = {\n",
    "    'num_filters': 32, 'filter_size': (3, 3),\n",
    "    'pad': 'same', \n",
    "    'pool_size': 2, 'pool': False\n",
    "}\n",
    "arch = lambda input_var: multiconvX #archX_oddfilter_variant(input_var, **archkws)\n",
    "archname = 'multiconvX_lrg' #.format(archkws['num_filters'])\n",
    "run_full_fit(multiconvX, archname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aggregation\n",
    "\n",
    "doesn't need run more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafilenames = ['0 (with groups)', '1 (with computer)', '2 (with computer)', '3 (with computer)', '4']\n",
    "datafilenames = [os.path.join(datadir, fname + '.csv') for fname in datafilenames]\n",
    "colnames = ['subject', 'color', 'bp', 'wp', 'zet', 'rt']\n",
    "\n",
    "e0 = pd.read_csv(datafilenames[0], names=colnames+['splitno'])\n",
    "e1 = pd.read_csv(datafilenames[1], names=colnames)\n",
    "e2 = pd.read_csv(datafilenames[2], names=colnames)\n",
    "e3 = pd.read_csv(datafilenames[3], names=colnames+['task', 'taskorder', 'session'])\n",
    "e4 = pd.read_csv(datafilenames[4], names=colnames+['timecondition'])\n",
    "Es = [e1, e2, e3, e4]\n",
    "for i, e in enumerate(Es[1:]):\n",
    "    e['subject'] = e['subject'] + Es[i-1].loc[Es[i-1]['subject']<1000, 'subject'].max()\n",
    "\n",
    "A = pd.concat([e[colnames] for e in [e1, e2, e3, e4]])\n",
    "\n",
    "groups = np.arange(len(A))%5 + 1\n",
    "np.random.seed(100001)\n",
    "np.random.shuffle(groups)\n",
    "A['group'] = groups\n",
    "\n",
    "A.to_csv(os.path.join(datadir, '1-4.csv'), encoding='ASCII', header=False, index=False)\n",
    "A.loc[A['subject']<1000, :].to_csv(\n",
    "    os.path.join(datadir, '1-4 (no computer).csv'), \n",
    "    encoding='ASCII', header=False, index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
