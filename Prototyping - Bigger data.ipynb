{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import lasagne\n",
    "import loading\n",
    "from training import *\n",
    "from network import *\n",
    "import architectures as arches\n",
    "\n",
    "# aliases\n",
    "L = lasagne.layers\n",
    "nl = lasagne.nonlinearities\n",
    "T = theano.tensor\n",
    "\n",
    "# directories\n",
    "headdir = os.path.expanduser('~/Google Drive/Bas Zahy Gianni - Games')\n",
    "paramsdir_ = os.path.join(headdir, 'Analysis/0_hvh/Params/nnets/')\n",
    "datadir = os.path.join(headdir, 'Data/model input')\n",
    "resultsdir = os.path.join(headdir, 'Analysis/0_hvh/Loglik/nnets')\n",
    "\n",
    "# loading data\n",
    "data = loading.default_loader(os.path.join(datadir, '1-4 (no computer).csv'))\n",
    "hvhdata = loading.default_loader(os.path.join(datadir, '0 (with groups).csv'))\n",
    "Xs = np.concatenate(hvhdata[2])\n",
    "ys = np.concatenate(hvhdata[3])\n",
    "Ss = np.concatenate(hvhdata[4])\n",
    "\n",
    "# load network specs\n",
    "with open('arch_specs.yaml') as archfile:\n",
    "    arch_dict = yaml.load(archfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name in ['g1', 'g2', 'g4', 'h1', 'h2', 'h4']:\n",
    "    paramsdir = os.path.join(paramsdir_, name[:-1])\n",
    "    architecture = arch_dict[name]\n",
    "\n",
    "    af = getattr(arches, architecture['type'])\n",
    "    net = af(**architecture['kwargs'])\n",
    "    print('N params:', L.count_params(net))\n",
    "    print(architecture)\n",
    "\n",
    "    run_full_fit(architecture, data, hvhdata, paramsdir=paramsdir, tune=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this is for another training scheme, using preassigned groups in both hvh and other data\n",
    "bulkdata_df = pd.concat([data[0], hvhdata[0]])\n",
    "bulkdata_df.to_csv(os.path.join(datadir, 'bulk.csv'), index=False, header=False)\n",
    "bulkdata = loading.default_loader(os.path.join(datadir, 'bulk.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "names = []\n",
    "test_errs = {}\n",
    "\n",
    "for letter in ['h', 'f', 'c']:\n",
    "    for idx in [4, 2, 1]:\n",
    "        names.append('{letter}{idx}'.format(letter=letter, idx=idx))\n",
    "\n",
    "for name in names:\n",
    "    paramsdir = os.path.join(paramsdir_, 'bulk_' + name[:-1])\n",
    "    os.makedirs(paramsdir, exist_ok=True)\n",
    "    \n",
    "    architecture = arch_dict[name]\n",
    "    archfunc = getattr(arches, arch_dict[name]['type'])\n",
    "    arch = lambda input_var=None: archfunc(input_var, **architecture['kwargs'])\n",
    "    trainer = DefaultTrainer(stopthresh=50, print_interval=25)\n",
    "\n",
    "    r = np.tile(np.arange(5), [5, 1])\n",
    "    r = (r + r.T) % 5\n",
    "\n",
    "    test_errs_ = []\n",
    "    filename_template = '{archname} bulk train {split_no} split.npz'\n",
    "    \n",
    "    for split in range(5):\n",
    "        filename = filename_template.format(archname=name, split_no=split)\n",
    "        \n",
    "        print(name, split)\n",
    "        train_idxs, val_idxs, test_idxs = r[split, :3], r[split, 3:4], r[split, 4:]\n",
    "\n",
    "        X, y = [np.concatenate(np.array(hvhdata[i])[train_idxs]) for i in [2, 3]]\n",
    "        X, y = [np.concatenate([Z, np.concatenate(np.array(data[i]))]) for Z, i in [(X, 2), (y, 3)]]\n",
    "        Xv, yv = [np.concatenate(np.array(hvhdata[i])[val_idxs]) for i in [2, 3]]\n",
    "        Xt, yt = [np.concatenate(np.array(hvhdata[i])[test_idxs]) for i in [2, 3]]\n",
    "\n",
    "        net = Network(arch)\n",
    "        net.save_params(os.path.join(paramsdir, filename))\n",
    "        trainer.train(net, (X, y), (Xv, yv))\n",
    "        err, acc, bats = trainer.test(net, (Xt, yt))\n",
    "        test_errs_.append(err / bats)\n",
    "        \n",
    "    test_errs[name] = test_errs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test_errs = {}\n",
    "for k, v in test_errs.items():\n",
    "    mean_test_errs[k] = np.mean(v)\n",
    "\n",
    "mean_test_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake news!\n",
    "\n",
    "Train on fake data from another model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add groups to fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd_ = pd.read_csv(os.path.join(datadir, 'fake news.csv'), names=['subject', 'color', 'bp', 'wp', 'zet', 'rt'])\n",
    "\n",
    "groups = np.arange(len(fd_)) % 5 + 1\n",
    "np.random.shuffle(groups)\n",
    "fd_['group'] = groups\n",
    "fd_.to_csv(os.path.join(datadir, 'fake news (with groups).csv'), encoding='ASCII', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit best networks on fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fake_data = loading.default_loader(os.path.join(datadir, 'fake news (with groups).csv'))\n",
    "trainer = DefaultTrainer(stopthresh=50, print_interval=25)\n",
    "name = 'h4'\n",
    "architecture = arch_dict[name]\n",
    "print(arch_dict[name])\n",
    "archfunc = getattr(arches, arch_dict[name]['type'])\n",
    "arch = lambda input_var=None: archfunc(input_var, **architecture['kwargs'])\n",
    "net_list = trainer.train_all(architecture=arch, data=fake_data, seed=985227)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_test_map = {0: 5, 1: 1, 2: 2, 3: 3, 4: 4}\n",
    "hd = hvhdata[0]\n",
    "hXs = hvhdata[2]\n",
    "hys = hvhdata[3]\n",
    "hidxs = hvhdata[1]\n",
    "hP = pd.DataFrame(index=hd.index, columns=list(range(36)))\n",
    "\n",
    "for i, net in enumerate(net_list):\n",
    "    test_group = idx_test_map[i]\n",
    "    test_loc = hd['group'] == test_group\n",
    "    \n",
    "    nll = net.itemized_test_fn(hXs[test_group-1], hys[test_group-1])\n",
    "    pred = net.output_fn(hXs[test_group-1])\n",
    "    hd.loc[test_loc, 'nll'] = nll\n",
    "    hP.loc[test_loc] = pred\n",
    "    \n",
    "    fname = '{} {} split fake data.npz'.format('bulk_h4', i)\n",
    "    net.save_params(os.path.join(paramsdir_, 'bulk_h4', fname))\n",
    "\n",
    "hd.to_csv(os.path.join(resultsdir, 'fake_nlls.csv'), index=False)\n",
    "hdmeans = hd.pivot_table(index='subject', values='nll')\n",
    "\n",
    "hdmeans.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = fake_data[0]\n",
    "fXs = fake_data[2]\n",
    "fys = fake_data[3]\n",
    "fidxs = fake_data[1]\n",
    "fP = pd.DataFrame(index=fd.index, columns=list(range(36)))\n",
    "\n",
    "for i, net in enumerate(net_list):\n",
    "    test_group = idx_test_map[i]\n",
    "    test_loc = fd['group'] == test_group\n",
    "    \n",
    "    nll = net.itemized_test_fn(fXs[test_group-1], fys[test_group-1])\n",
    "    pred = net.output_fn(fXs[test_group-1])\n",
    "    fd.loc[test_loc, 'nll'] = nll\n",
    "    fP.loc[test_loc] = pred\n",
    "    \n",
    "    fname = '{} {} split fake data.npz'.format('fake_h4', i)\n",
    "    net.save_params(os.path.join(paramsdir_, 'fake_h4', fname))\n",
    "\n",
    "fd.to_csv(os.path.join(resultsdir, 'fake_nlls.csv'), index=False)\n",
    "fdmeans = fd.pivot_table(index='subject', values='nll')\n",
    "\n",
    "fdmeans.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fP.to_csv(os.path.join(resultsdir, 'fake_predictions.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subject tuning\n",
    "\n",
    "Usually doesn't work (not enough data even for very simple classifier layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dafiname = os.path.join(datadir, '0 (with groups).csv')\n",
    "subject_data = [loading.default_loader(dafiname, subject=s) for s in range(40)]\n",
    "arch = archs[archname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print([len(s[0]) for s in subject_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    pafiname = '{} {} split agg fit exp 1-4.npz'.format(archname, i)\n",
    "    prenet = Network(arch)\n",
    "    prenet.load_params(os.path.join(paramsdir, pafiname))\n",
    "    params = L.get_all_param_values(prenet.net)\n",
    "    print('PREFIT {}\\n'.format(i))\n",
    "    \n",
    "    for s in range(40):\n",
    "        sdata = subject_data[s]\n",
    "        num_obs = len(sdata[0])\n",
    "        bs = num_obs//5\n",
    "#         if num_obs > 50:\n",
    "        tuner = FineTuner(stopthresh=10, batchsize=bs)\n",
    "        print('SUBJECT {}\\n'.format(s))\n",
    "        \n",
    "        for j in range(5):\n",
    "            fname = '{} {} agg fit exp 1-4 {} subject {} tune fit exp 0'.format(archname, i, s, j)\n",
    "            net = tuner.train_all(architecture=arch, data=sdata, split=j, startparams=params, freeze=True)\n",
    "            net.save_params(os.path.join(paramsdir, fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aggregation\n",
    "\n",
    "doesn't need run more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafilenames = ['0 (with groups)', '1 (with computer)', '2 (with computer)', '3 (with computer)', '4']\n",
    "datafilenames = [os.path.join(datadir, fname + '.csv') for fname in datafilenames]\n",
    "colnames = ['subject', 'color', 'bp', 'wp', 'zet', 'rt']\n",
    "\n",
    "e0 = pd.read_csv(datafilenames[0], names=colnames+['splitno'])\n",
    "e1 = pd.read_csv(datafilenames[1], names=colnames)\n",
    "e2 = pd.read_csv(datafilenames[2], names=colnames)\n",
    "e3 = pd.read_csv(datafilenames[3], names=colnames+['task', 'taskorder', 'session'])\n",
    "e4 = pd.read_csv(datafilenames[4], names=colnames+['timecondition'])\n",
    "Es = [e1, e2, e3, e4]\n",
    "for i, e in enumerate(Es[1:]):\n",
    "    e['subject'] = e['subject'] + Es[i-1].loc[Es[i-1]['subject']<1000, 'subject'].max()\n",
    "\n",
    "A = pd.concat([e[colnames] for e in [e1, e2, e3, e4]])\n",
    "\n",
    "groups = np.arange(len(A))%5 + 1\n",
    "np.random.seed(100001)\n",
    "np.random.shuffle(groups)\n",
    "A['group'] = groups\n",
    "\n",
    "A.to_csv(os.path.join(datadir, '1-4.csv'), encoding='ASCII', header=False, index=False)\n",
    "A.loc[A['subject']<1000, :].to_csv(\n",
    "    os.path.join(datadir, '1-4 (no computer).csv'), \n",
    "    encoding='ASCII', header=False, index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
