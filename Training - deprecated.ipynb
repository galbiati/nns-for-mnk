{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This and other notebooks are scratch pads to try ideas out or debug small features before integrating with more proper scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Archs - prototypes\n",
    "\n",
    "More archs are imported from archs.py\n",
    "\n",
    "**Wish list**: equivalent to $NT + C_{opp}$\n",
    "\n",
    "How this will work could be tetchy: must be at least loosely interpretable as having to do with game theoretic value, but value is not reflected in network output. \n",
    "\n",
    "However, we can separate softmax nonlinearity and apply rectified linear instead to get a \"value\" representation for each position (!). Then, we can repeat with same network by (1) adding argmax to input, (2) swapping input channels, and (3) passing back through the network. We can then do one of several things (optionally?) scale the negative opponent response, add, and softmax\n",
    "\n",
    "Another option is to change (pseudo-)output layer to be a 72 vector to hold opponent values for true min-maxing. This might actually be better, but is more difficult to compare to old architecture. \n",
    "\n",
    "**Wish list**: non random init to HS-like features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def thinky_agent(nfil=32, input_var=None):\n",
    "    \"\"\"Theano graph for convnet that adds opponent; WIP\"\"\"\n",
    "    \n",
    "    class FixLayer(lasagne.layers.Layer):\n",
    "        def get_output_for(self, input, **kwargs):\n",
    "            corrector = (1 - input_var.sum(axis=1)).reshape((input_var.shape[0], 36))\n",
    "            numer = input * corrector\n",
    "            return numer / numer.sum(axis=1).dimshuffle((0, 'x'))\n",
    "        \n",
    "    class ValueLayer(lasagne.layers.ElemwiseSumLayer):\n",
    "        def get_output_for(self, inputs, **kwargs):\n",
    "            pass\n",
    "        \n",
    "        def get_output_shape_for(self, input_shapes):\n",
    "            pass\n",
    "        \n",
    "    input_shape=(None, 2, 4, 9)\n",
    "    input_layer = lasagne.layers.InputLayer(shape=input_shape, input_var=input_var)\n",
    "    \n",
    "    network = lasagne.layers.Conv2DLayer(\n",
    "        input_layer, \n",
    "        num_filters=nfil, filter_size=(4,4), pad='full',\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform()\n",
    "    )\n",
    "    \n",
    "    network = lasagne.layers.DropoutLayer(network, p=.75)\n",
    "    \n",
    "    network = lasagne.layers.NonlinearityLayer(\n",
    "        network, \n",
    "        nonlinearity=lasagne.nonlinearities.softmax\n",
    "    )\n",
    "    \n",
    "    network = FixLayer(network)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: it is not possible with lasagne to concat with different filter shapes :(\n",
    "#     pool_layer_1_1 = L.layers.FeaturePoolLayer(conv_layer_1_1, pool_size=2)\n",
    "    \n",
    "#     conv_layer_1_2 = L.layers.Conv2DLayer(\n",
    "#         input_layer, num_filters=32, filter_size=(4,8), pad='full', \n",
    "#         nonlinearity=L.nonlinearities.leaky_rectify,\n",
    "#         W=lasagne.init.HeUniform(gain='relu'),\n",
    "#     )\n",
    "    \n",
    "#     pool_layer_1_2 = L.layers.FeaturePoolLayer(conv_layer_1_2, pool_size=2, axis=-1)\n",
    "    \n",
    "    \n",
    "#     layer_1 = L.layers.ConcatLayer([pool_layer_1_1, pool_layer_1_2])\n",
    "    \n",
    "#     pool_layer_1 = L.layers.MaxPool2DLayer(conv_layer_1, pool_size=(2, 2), stride=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne as L\n",
    "import pandas as pd\n",
    "from util import *\n",
    "from archs import *\n",
    "from load_data import *\n",
    "import train as tr\n",
    "from network import MNKNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "headdir = os.path.expanduser('~/Google Drive/Bas Zahy Gianni - Games')\n",
    "datafile = os.path.join(headdir, 'Data/0_hvh/Clean/_summaries/model_input_with_groups.csv')\n",
    "resultsdir = os.path.join(headdir, 'Analysis/0_hvh/Loglik/nns')\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('poster')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prototype(nfil=None, input_var=None, return_layers=False):\n",
    "    input_shape = (None, 2, 4, 9)\n",
    "    FixLayer = make_FixLayer(input_var)\n",
    "    \n",
    "    layers = []\n",
    "    \n",
    "    input_layer = L.layers.InputLayer(shape=input_shape, input_var=input_var)\n",
    "    \n",
    "    conv_layer_1 = L.layers.Conv2DLayer(\n",
    "        input_layer, num_filters=128, filter_size=(2, 2), pad='full', \n",
    "        nonlinearity=L.nonlinearities.leaky_rectify,\n",
    "        W=L.init.HeUniform(gain='relu'),\n",
    "    )\n",
    "    \n",
    "    conv_layer_1 = L.layers.DropoutLayer(conv_layer_1, p=.75)\n",
    "    \n",
    "    pool_layer_1 = L.layers.FeaturePoolLayer(conv_layer_1, pool_size=2)\n",
    "    \n",
    "    \n",
    "    dense_layer_1 = L.layers.DenseLayer(\n",
    "        pool_layer_2, num_units=128, nonlinearity=L.nonlinearities.leaky_rectify,\n",
    "        W = L.init.HeUniform(gain='relu')\n",
    "    )\n",
    "    \n",
    "    dense_layer_2 = L.layers.DenseLayer(\n",
    "        dense_layer_1, num_units=36, nonlinearity=L.nonlinearities.leaky_rectify,\n",
    "        W = L.init.HeUniform(gain='relu')\n",
    "    )\n",
    "    \n",
    "    softmax_output = L.layers.NonlinearityLayer(dense_layer_2, nonlinearity=L.nonlinearities.softmax)\n",
    "    network = FixLayer(softmax_output)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prototype(nfil=None, input_var=None, return_layers=False):\n",
    "    input_shape = (None, 2, 4, 9)\n",
    "    FixLayer = make_FixLayer(input_var)\n",
    "        \n",
    "    input_layer = L.layers.InputLayer(shape=input_shape, input_var=input_var)\n",
    "    \n",
    "    network = L.layers.Conv2DLayer(\n",
    "        input_layer, num_filters=32, filter_size=(4, 4), pad='full', \n",
    "        nonlinearity=L.nonlinearities.leaky_rectify,\n",
    "        W=L.init.HeUniform(gain='relu'),\n",
    "    )\n",
    "        \n",
    "#     network = L.layers.FeaturePoolLayer(network, pool_function=T.sum, pool_size=2)\n",
    "    \n",
    "    network = L.layers.DropoutLayer(network, p=.75)\n",
    "    \n",
    "#     network = L.layers.DenseLayer(\n",
    "#         network, num_units=128, nonlinearity=L.nonlinearities.leaky_rectify,\n",
    "#         W=L.init.HeUniform(gain='relu')\n",
    "#     )\n",
    "    \n",
    "#     network = L.layers.DropoutLayer(network, p=.7)\n",
    "    \n",
    "    network = L.layers.DenseLayer(\n",
    "        network, num_units=36, nonlinearity=L.nonlinearities.leaky_rectify,\n",
    "        W = L.init.HeUniform(gain='relu')\n",
    "    )\n",
    "    \n",
    "    softmax_output = L.layers.NonlinearityLayer(network, nonlinearity=L.nonlinearities.softmax)\n",
    "    network = FixLayer(softmax_output)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single architecture training\n",
    "\n",
    "For prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(985227)\n",
    "\n",
    "ARCH = prototype\n",
    "LVL = 32 #None for softmax\n",
    "arch = lambda input_var=None: ARCH(nfil=LVL, input_var=input_var)\n",
    "\n",
    "CV_nlls, traces, nets = tr.CV_train(\n",
    "    arch, \n",
    "    batchsize=128, \n",
    "    epochs=5000, \n",
    "    thresh=100,\n",
    "    everyn=50,\n",
    "    custom_loaded=CV_loader(datafile, norm_input=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = nets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n.save_params('sample_network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qvar = T.dmatrix('dist')\n",
    "pvar = T.ivector('targ')\n",
    "loss = lasagne.objectives.categorical_crossentropy(qvar, pvar)\n",
    "loss_fn = theano.function([qvar, pvar], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nets[0].data = nets[0].data.drop(d.columns[~d.columns.isin(['subject', 'color', 'bp', 'wp', 'response', 'rt', 'splitg'])], axis=1)\n",
    "d = nets[0].data\n",
    "d.loc[d['color']==0, 'own'] = d.loc[d['color']==0, 'bp']\n",
    "d.loc[d['color']==0, 'opp'] = d.loc[d['color']==0, 'wp']\n",
    "d.loc[d['color']==1, 'own'] = d.loc[d['color']==1, 'wp']\n",
    "d.loc[d['color']==1, 'opp'] = d.loc[d['color']==1, 'bp']\n",
    "for j in range(36):\n",
    "    d[str(j)] = np.nan\n",
    "\n",
    "for i in np.arange(1, 6, 1):\n",
    "    c = d['splitg']==i\n",
    "    dd = d.loc[c]\n",
    "    dp = dd['own'] + dd['opp']\n",
    "    dp = dp.map(lambda x: np.array(list(x)).astype(int).reshape([1, 2, 4, 9]))\n",
    "    dp = np.concatenate(dp.values)\n",
    "    net_output = nets[i-1].output_fn(dp)\n",
    "    d.loc[c, [str(j) for j in range(36)]] = net_output   \n",
    "    d.loc[c, 'nll'] = loss_fn(net_output, d.loc[c, 'response'])\n",
    "    \n",
    "d[['subject', 'splitg', 'nll', 'response'] + [str(i) for i in range(36)]].to_csv(os.path.join(resultsdir, 'smart_32.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import bayes_mvs\n",
    "bayes_mvs(d['nll'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture comparison\n",
    "\n",
    "**Don't run this section** unless you want to train 18 different networks. Load params or remove loop to train the architecture you want instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(985227)\n",
    "datafile = '../../Google Drive/Bas Zahy Gianni - Games/Data/0_hvh/Clean/_summaries/model_input_with_groups.csv'\n",
    "\n",
    "CVN, TRACES, NETS = ([],)*3\n",
    "ARCHS = [naive_agent, smart_agent]\n",
    "ARCH_NAMES = ['naive', 'smart']\n",
    "LVLS = 2**np.arange(9)\n",
    "\n",
    "\n",
    "for lvl in LVLS:\n",
    "    for aidx, arch in enumerate(ARCHS):\n",
    "        np.random.seed(985227)\n",
    "        CV_nlls, traces, nets = tr.CV_train(\n",
    "            lambda input_var=None: arch(nfil=lvl, input_var=input_var),\n",
    "            batchsize=500, \n",
    "            epochs=5000, \n",
    "            thresh=100,\n",
    "            everyn=5000,\n",
    "            custom_loaded=custom_loader(datafile)\n",
    "        )\n",
    "        \n",
    "        CVN.append(CV_nlls)\n",
    "        TRACES.append(traces)\n",
    "        NETS.append(nets)\n",
    "        \n",
    "        print(\n",
    "            '\\n',\n",
    "            ARCH_NAMES[aidx] + ' ' + 'LVL' + ' ' + str(lvl) + ':\\t',\n",
    "            np.mean(CV_nlls),\n",
    "            '\\n'\n",
    "        )\n",
    "        \n",
    "        save_appended_data(\n",
    "            nets, \n",
    "            ARCH_NAMES[aidx] + '_' + str(lvl)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save traces and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CVN = CVN[::3]\n",
    "TRACES = TRACES[1::3]\n",
    "NETS = NETS[2::3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, t in enumerate(TRACES):\n",
    "    np.savetxt(\n",
    "        './results/trace_' + str(i) + '.csv', \n",
    "        np.array(t).reshape([15, 5000]).T, \n",
    "        delimiter=','\n",
    "    )\n",
    "    \n",
    "for i, n in enumerate(NETS):\n",
    "    for j, nn in enumerate(n):\n",
    "        nn.save_params('./results/params_' + str(i*len(nets) + j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some unorganized junk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datadir = '../../Google Drive/Bas Zahy Gianni - Games/Data/0_hvh/Clean/_summaries/'\n",
    "datafile = datadir + 'model_input_with_groups.csv'\n",
    "datatypes = [('subject', 'i4'), ('color', 'i4'), \n",
    "                 ('bp', 'S36'), ('wp', 'S36'), \n",
    "                 ('response', 'i4'), ('rt', 'i4'), ('splitg', 'i4')]\n",
    "data = np.loadtxt(datafile, delimiter=',', dtype=datatypes)\n",
    "data = pd.DataFrame.from_records(data)\n",
    "decoder = lambda x: x.decode('utf-8')\n",
    "data.loc[:, 'bp'] = data.loc[:, 'bp'].map(decoder)\n",
    "data.loc[:, 'wp'] = data.loc[:, 'wp'].map(decoder)\n",
    "# data.loc[data.subject==0, :].to_csv(datadir + 'sub0.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Params (if you skipped training)\n",
    "\n",
    "(to do! pretty easy though)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save transferable params\n",
    "\n",
    "(needs updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_param_values(n.network)\n",
    "\n",
    "for i, p in enumerate(params):\n",
    "    ps = p.shape\n",
    "    pp = p.reshape([ps[0], np.product(ps[1:])])\n",
    "    np.savetxt(str(i//2) + str(i%2)+'b', pp, fmt='%.10f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt('./results/smart_32_traces_training.txt', np.array(traces)[:, 0, :1000].reshape((5, 1000)).T)\n",
    "np.savetxt('./results/smart_32_traces_validate.txt', np.array(traces)[:, 1, :1000].reshape((5, 1000)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(traces).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(8.5, 5.5))\n",
    "axes.plot(traces[0][0][:500])\n",
    "axes.plot(traces[1][1][:500])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: move to the results notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agg_nlls(data):\n",
    "    meanagg = pd.pivot_table(data=data, \n",
    "                   values='cnn_nll', index='n_pieces', \n",
    "                   aggfunc=np.mean)\n",
    "    sdagg = pd.pivot_table(data=data,\n",
    "                          values='cnn_nll', index='n_pieces',\n",
    "                          aggfunc=np.std)\n",
    "    return meanagg, sdagg\n",
    "\n",
    "figs, axes = plt.subplots(10, 4, figsize=(18,24), \n",
    "                          sharex=True, sharey=True)\n",
    "\n",
    "for p in data.subject.unique():\n",
    "    ax = axes.flatten()[p]\n",
    "    m, sd = agg_nlls(data.loc[data.subject==p])\n",
    "    ax.errorbar(m.index, m.values, yerr=sd.values)\n",
    "    ax.set_ylim([0,8])\n",
    "    \n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m, sd = agg_nlls(data)\n",
    "plt.errorbar(m.index, m.values, yerr=sd.values, ecolor=(.8, .9, .8))\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0,4.2])\n",
    "ax.set_xlim([-.5, 35.5])\n",
    "sns.despine()\n",
    "fig = plt.gcf()\n",
    "fig.savefig('./nll_against_n_pieces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos = 506\n",
    "print(data.loc[pos, 'subject'])\n",
    "fig, axes = plt.subplots(1, 1, figsize=(20, 10))\n",
    "d = data.loc[pos, positions].values.reshape([4,9]).astype(float)\n",
    "print(d)\n",
    "sns.heatmap(\n",
    "    d, square=True, vmin=0, vmax=.25, cbar=False,\n",
    "    xticklabels=False, yticklabels=False, ax=axes, annot=True\n",
    ")\n",
    "\n",
    "reconstitute = lambda x: np.array(list(map(int, x))).reshape(4,9)\n",
    "\n",
    "if data.loc[pos, 'color'] == 0:\n",
    "    own_color, opp_color = 'black', 'white'\n",
    "else:\n",
    "    own_color, opp_color = 'white', 'black'\n",
    "\n",
    "p = np.where(reconstitute(data.loc[pos, 'bp'])==1)\n",
    "plt.scatter( .5 + p[1], 3.5 - p[0], c=own_color, s=10000)\n",
    "p = np.where(reconstitute(data.loc[pos, 'wp'])==1)\n",
    "plt.scatter( .5 + p[1], 3.5 - p[0], c=opp_color, s=10000)\n",
    "r = data.loc[pos, 'response']\n",
    "p = (r % 9, r // 9)\n",
    "plt.scatter(p[0] + .5, 3.5 - p[1], c=(.7, .7, .7), s=10000);\n",
    "fig.savefig('nice example.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10,6), squeeze=False)\n",
    "ep = net.last_epoch\n",
    "ax = axes[0,0]\n",
    "ax.plot(np.arange(net.tr_nll.size)[:ep], net.tr_nll[:ep]/33, \n",
    "         label='Training set NLL', color='grey')\n",
    "ax.plot(np.arange(net.val_nll.size)[:ep], net.val_nll[:ep]/5, \n",
    "         label='Validation set NLL', color='black')\n",
    "ax.plot([0, 200], [3.58, 3.58], 'k--', label='Random guessing')\n",
    "lgd = ax.legend(bbox_to_anchor=(1.45, .65))\n",
    "ax.set_xlabel('Training Epoch')\n",
    "ax.set_ylabel('NLL')\n",
    "ax.set_title('Overfitting on combined data')\n",
    "sns.despine()\n",
    "fig.savefig('training_slope.png', bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(64, 2, figsize=(10, 192), squeeze=False)\n",
    "\n",
    "filters = lasagne.layers.get_all_param_values(n.network)[0]\n",
    "\n",
    "for idx in range(64):\n",
    "    sns.heatmap(filters[idx,0], square=True, cmap='Greys', fmt= '.2f', \n",
    "                vmin=-1.5, vmax=1.5, annot=True,\n",
    "                ax=axes[idx,0], cbar=False, xticklabels=False, yticklabels=False)\n",
    "    sns.heatmap(filters[idx,1], square=True, cmap='Greys', fmt='.2f', \n",
    "                vmin=-1.5, vmax=1.5, annot=True,\n",
    "                ax=axes[idx,1], cbar=False, xticklabels=False, yticklabels=False)\n",
    "\n",
    "fig.savefig('filters.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
