{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "This notebook is to help split the workstream for model fitting and analyses, evaluations, and visualizations. It should be able to load any previously fitted network, so long as the parameters were saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import lasagne\n",
    "import loading\n",
    "from training import *\n",
    "from network import *\n",
    "from architectures import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import bayes_mvs, entropy, linregress, spearmanr\n",
    "\n",
    "# settings\n",
    "sns.set_style('white')\n",
    "sns.set_context('poster')\n",
    "colors = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# aliases\n",
    "L = lasagne.layers\n",
    "nl = lasagne.nonlinearities\n",
    "T = theano.tensor\n",
    "bmvs = bayes_mvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "For now, we just want the data from the first experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headdir = os.path.expanduser('~/Google Drive/Bas Zahy Gianni - Games')\n",
    "paramsdir = os.path.join(headdir, 'Analysis/0_hvh/Params/nnets/temp')\n",
    "datadir = os.path.join(headdir, 'Data/model input')\n",
    "resultsdir = os.path.join(headdir, 'Analysis/0_hvh/Loglik/nnets')\n",
    "\n",
    "data = loading.default_loader(os.path.join(datadir, '1-4 (no computer).csv'))\n",
    "hvhdata = loading.default_loader(os.path.join(datadir, '0 (with groups).csv'))\n",
    "df = hvhdata[0]\n",
    "Xs = np.concatenate(hvhdata[2])\n",
    "ys = np.concatenate(hvhdata[3])\n",
    "Ss = np.concatenate(hvhdata[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Way too much copypaste below; clean this up into functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_filters = [4, 8, 16, 32, 64, 128]\n",
    "archnames = [\"arch{}\".format(n) for n in num_filters]\n",
    "columns = pd.MultiIndex.from_product([archnames, np.arange(5).astype(str)])\n",
    "tune_tidy = pd.DataFrame(index=df.index, columns=columns)\n",
    "tune_tidy['subject'] = df['subject']\n",
    "tune_tidy['group'] = df['group'] - 1\n",
    "\n",
    "pretrain_tidy = pd.DataFrame(index=df.index, columns=columns)\n",
    "pretrain_tidy['subject'] = df['subject']\n",
    "pretrain_tidy['group'] = df['group'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for prefit_idx in range(5):\n",
    "    for test_split in range(5):\n",
    "        # create network container with nfil filters architecture\n",
    "        fname = 'multiconvX {} agg fit exp 1-4 {} tune fit exp 0.npz'.format(prefit_idx, test_split)\n",
    "\n",
    "        arch = multiconvX\n",
    "        net = Network(arch)\n",
    "        net.load_params(os.path.join(paramsdir, fname))\n",
    "\n",
    "        group_idx = (test_split-1)%5 # fuck this up if you want to see overfitting\n",
    "        test_data = df.loc[df['group']==(group_idx+1)]\n",
    "        Xt, yt, St, Gt, Npt = loading.unpack_data(test_data)\n",
    "        res = net.itemized_test_fn(Xt, yt)\n",
    "        l1 = 'multiconvX'\n",
    "        l2 = str(prefit_idx)\n",
    "        tune_tidy.loc[tune_tidy['group']==group_idx, (l1, l2)] = res\n",
    "\n",
    "    fname = 'multiconvX {} split agg fit exp 1-4.npz'.format(prefit_idx)\n",
    "\n",
    "    arch = multiconvX\n",
    "    net = Network(arch)\n",
    "    net.load_params(os.path.join(paramsdir, fname))\n",
    "    Xt, yt, St, Gt, Npt = loading.unpack_data(df)\n",
    "    res = net.itemized_test_fn(Xt, yt)\n",
    "    l1 = 'multiconvX'\n",
    "    l2 = str(prefit_idx)\n",
    "    pretrain_tidy[(l1, l2)] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array([i.size for i in L.get_all_param_values(net.net)]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pt = pretrain_tidy[['subject', 'group']].copy()\n",
    "pt.columns = pt.columns.get_level_values(0)\n",
    "pt['multiconvX'] = pretrain_tidy['multiconvX'].mean(axis=1)\n",
    "pt.pivot_table(index='subject', values='multiconvX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for nfil in num_filters:\n",
    "    for prefit_idx in range(5):\n",
    "        for test_split in range(5):\n",
    "            # create network container with nfil filters architecture\n",
    "            if nfil == 32:\n",
    "                fname = '{} agg fit exp 1-4 {} tune fit exp 0.npz'.format(prefit_idx, test_split)\n",
    "            else:\n",
    "                fname = 'arch{} {} agg fit exp 1-4 {} tune fit exp 0.npz'.format(nfil, prefit_idx, test_split)\n",
    "            \n",
    "            arch = lambda input_var: archX(input_var, num_filters=nfil)\n",
    "            net = Network(arch)\n",
    "            net.load_params(os.path.join(paramsdir, fname))\n",
    "            \n",
    "            group_idx = (test_split-1)%5 # fuck this up if you want to see overfitting\n",
    "            test_data = df.loc[df['group']==(group_idx+1)]\n",
    "            Xt, yt, St, Gt, Npt = loading.unpack_data(test_data)\n",
    "            res = net.itemized_test_fn(Xt, yt)\n",
    "            l1 = 'arch{}'.format(nfil)\n",
    "            l2 = str(prefit_idx)\n",
    "            tune_tidy.loc[tune_tidy['group']==group_idx, (l1, l2)] = res\n",
    "\n",
    "        if nfil == 32:\n",
    "            fname = '{} split agg fit exp 1-4.npz'.format(prefit_idx)\n",
    "        else:\n",
    "            fname = 'arch{} {} split agg fit exp 1-4.npz'.format(nfil, prefit_idx)\n",
    "            \n",
    "        arch = lambda input_var: archX(input_var, num_filters=nfil)\n",
    "        net = Network(arch)\n",
    "        net.load_params(os.path.join(paramsdir, fname))\n",
    "        Xt, yt, St, Gt, Npt = loading.unpack_data(df)\n",
    "        res = net.itemized_test_fn(Xt, yt)\n",
    "        l1 = 'arch{}'.format(nfil)\n",
    "        l2 = str(prefit_idx)\n",
    "        pretrain_tidy[(l1, l2)] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pivtidy(tidydf):\n",
    "    t = tidydf[archnames].astype(float)\n",
    "    t = t.mean(axis=1, level=0)\n",
    "    t['subject'] = tidydf['subject']\n",
    "    t['group'] = tidydf['group']\n",
    "    tpiv = t.pivot_table(index='subject', values=archnames)\n",
    "    return tpiv\n",
    "\n",
    "pretrain_piv = pivtidy(pretrain_tidy)\n",
    "tune_piv = pivtidy(tune_tidy)\n",
    "\n",
    "print('pretrain agg\\n')\n",
    "for arc in archnames:\n",
    "    print(arc, \"\\n\", bmvs(pretrain_tidy[arc].values), \"\\n\")\n",
    "\n",
    "print('\\npretrain sub\\n')\n",
    "for arc in archnames:\n",
    "    print(arc, \"\\n\", bmvs(pretrain_piv[arc].values), \"\\n\")\n",
    "\n",
    "print('\\ntune agg\\n')\n",
    "for arc in archnames:\n",
    "    print(arc, \"\\n\", bmvs(tune_tidy[arc].values), \"\\n\")\n",
    "\n",
    "print('\\ntune sub\\n')\n",
    "for arc in archnames:\n",
    "    print(arc, \"\\n\", bmvs(tune_piv[arc].values), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = 'multiconvX 4 split agg fit exp 1-4.npz'\n",
    "with np.load(os.path.join(paramsdir, fname)) as loaded:\n",
    "    for k, v in loaded.items():\n",
    "        print(k, v.shape)\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    params_list = [(i[0], i[1]) for i in loaded.items()]\n",
    "    params_order = np.array([i[0][4:6] for i in params_list]).astype(int)\n",
    "    o = list(params_order.argsort())\n",
    "    print(list(o))\n",
    "    for i, j in [params_list[k] for k in o]:\n",
    "        print(i, j.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nfil = 64\n",
    "archname = 'arch{}'.format(nfil)\n",
    "fname = '{} 0 split agg fit exp 1-4.npz'.format(archname)\n",
    "arch = lambda input_var: archX(input_var, num_filters=64)\n",
    "net = Network(arch)\n",
    "net.load_params(os.path.join(paramsdir, fname))\n",
    "defarray = np.loadtxt(os.path.join(headdir, 'Analysis/0_hvh/Loglik/loglik_hvh_default.txt'))\n",
    "defarray = defarray.reshape([40, 5, 10]).reshape([40, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histkws = {\n",
    "    'alpha': .8, 'edgecolor': 'white',\n",
    "    'normed': False, 'bins': np.arange(1.2, 3.0, .1)   \n",
    "}\n",
    "\n",
    "scatterkws = {\n",
    "    'marker':'o', 'markersize':7, 'linestyle': 'None', 'alpha': .8\n",
    "}\n",
    "\n",
    "hmkws = {\n",
    "    'cbar': False, 'cmap': sns.blend_palette([(.95, .95, .95), colors[0]], n_colors=16, as_cmap=True),\n",
    "    'square': True, 'xticklabels': False, 'yticklabels': False,\n",
    "    'vmin': 0, 'vmax': 1\n",
    "}\n",
    "\n",
    "boardplotkws = {\n",
    "    'marker': 'o', 'markersize': 20, 'markeredgecolor': 'black', 'markeredgewidth': 2,\n",
    "    'linestyle': 'None'\n",
    "}\n",
    "\n",
    "def show_net_response(pos_idx, ax, net=net):\n",
    "    response = net.output_fn(Xs[pos_idx:pos_idx+1, :, :, :])\n",
    "    sns.heatmap(response.reshape([4, 9])[::-1, :], ax=ax, **hmkws)\n",
    "    \n",
    "    if Xs[pos_idx, :, :, :].sum()%2 == 0:\n",
    "        b = 0\n",
    "        w = 1\n",
    "    else:\n",
    "        b = 1\n",
    "        w = 0\n",
    "    \n",
    "    bcoords = np.where(Xs[pos_idx, b, :, :]==1)\n",
    "    wcoords = np.where(Xs[pos_idx, w, :, :]==1)\n",
    "    rcoords = np.unravel_index(ys[pos_idx], (4, 9))\n",
    "\n",
    "    ax.plot(bcoords[1]+.5, bcoords[0]+.5, color='black', **boardplotkws)\n",
    "    ax.plot(wcoords[1]+.5, wcoords[0]+.5, color='white', **boardplotkws)\n",
    "    ax.plot(rcoords[1]+.5, rcoords[0]+.5, color=colors[2], **boardplotkws)\n",
    "    plt.setp(ax, frame_on=False)\n",
    "    \n",
    "    return None\n",
    "\n",
    "sns.palplot(sns.blend_palette([(.95, .95, .95), colors[0]], n_colors=16, as_cmap=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean this mess up tomorrow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def countpieces(row):\n",
    "    bp = row['bp']\n",
    "    wp = row['wp']\n",
    "    p = np.array(list(bp+wp)).astype(int)\n",
    "    return p.sum()\n",
    "pretrain_tidy['npieces'] = df.apply(countpieces, axis=1)\n",
    "ptt = pretrain_tidy[[archname, 'subject', 'group', 'npieces']].mean(axis=1, level=0)\n",
    "ptt_piecepiv = ptt.pivot_table(index='npieces', values=archname, columns='subject', aggfunc=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(12, 14), squeeze=False)\n",
    "\n",
    "ax = axes[0, 0]\n",
    "ax.hist(pretrain_piv[archname].values, color=colors[0], label='Convnet', **histkws)\n",
    "ax.hist(defarray.mean(axis=1), color=colors[1], label='H search', **histkws)\n",
    "ax.legend(loc=0)\n",
    "plt.setp(ax, xlabel='CV NLL', ylabel='# Subjects')\n",
    "\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ptemp = pretrain_tidy[['subject', archname]]\n",
    "mos = [bmvs(ptemp.loc[ptemp['subject']==i, archname].values, alpha=.95) for i in np.arange(40)]\n",
    "means = np.array([mo[0][0] for mo in mos])\n",
    "lbs = np.array([mo[0][1][0] for mo in mos])\n",
    "ubs = np.array([mo[0][1][1] for mo in mos])\n",
    "\n",
    "dmos = [bmvs(defarray[i, :], alpha=.95) for i in np.arange(40)]\n",
    "dmeans = np.array([mo[0][0] for mo in dmos])\n",
    "dlbs = np.array([mo[0][1][0] for mo in dmos])\n",
    "dubs = np.array([mo[0][1][1] for mo in dmos])\n",
    "\n",
    "orderidx = defarray.mean(axis=1).argsort()\n",
    "\n",
    "ax.plot(np.arange(40), dmeans[orderidx], color=colors[1], **scatterkws)\n",
    "ax.fill_between(np.arange(40), y1=dlbs[orderidx], y2=dubs[orderidx], alpha=.25, color=colors[1])\n",
    "ax.plot(np.arange(40), means[orderidx], color=colors[0], **scatterkws)\n",
    "plt.setp(ax, xlabel='Subject (ranked by default fit)', ylabel='CV NLL')\n",
    "\n",
    "\n",
    "ax = axes[1, 0]\n",
    "mos = [bmvs(ptt_piecepiv.loc[i, :].values, alpha=.95) for i in np.arange(36)]\n",
    "means = np.array([mo[0][0] for mo in mos])\n",
    "lbs = np.array([mo[0][1][0] for mo in mos])\n",
    "ubs = np.array([mo[0][1][1] for mo in mos])\n",
    "\n",
    "means_corrected = -(means + np.log(1/(np.arange(36)+1)[::-1]))\n",
    "ax.plot(np.arange(36), -means) # **scatterkws)\n",
    "# ax.fill_between(np.arange(36), y1=lbs, y2=ubs)\n",
    "plt.setp(ax, xlabel='# pieces', ylabel='CV NLL')\n",
    "\n",
    "\n",
    "ax = axes[1, 1]\n",
    "blank = np.zeros([1, 2, 4, 9])\n",
    "sns.heatmap(net.output_fn(blank).reshape([4, 9])[::-1, :], ax=ax, **hmkws)\n",
    "plt.setp(ax, frame_on=False)\n",
    "\n",
    "\n",
    "ax = axes[2, 0]\n",
    "show_net_response(0, ax=ax)\n",
    "\n",
    "\n",
    "ax = axes[2, 1]\n",
    "show_net_response(12, ax=ax)\n",
    "\n",
    "\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the filters like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filter_layer = L.get_all_layers(subnet.net)[1]\n",
    "filter_output = L.get_output(filter_layer, deterministic=True)\n",
    "scaled_foutput = L.get_output(L.get_all_layers(subnet.net)[2], deterministic=True)\n",
    "pooled_foutput = L.get_output(L.get_all_layers(subnet.net)[3], deterministic=True)\n",
    "\n",
    "filter_output_fn = theano.function([subnet.input_var], filter_output)\n",
    "scaled_foutput_fn = theano.function([subnet.input_var], scaled_foutput)\n",
    "pooled_foutput_fn = theano.function([subnet.input_var], pooled_foutput)\n",
    "\n",
    "filters = L.get_all_param_values(filter_layer)[0]\n",
    "\n",
    "imshowkws = {\n",
    "    'interpolation': 'nearest',\n",
    "#     'vmin': -1, 'vmax': 1,\n",
    "    'cmap': sns.diverging_palette(20, 240, n=15, s=99, as_cmap=True)\n",
    "}\n",
    "\n",
    "def show_filter_output(pos_idx, func=filter_output_fn, filter_idx=None, ax=None, imshowkws=imshowkws):\n",
    "    if not ax:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    if not (filter_idx is None):\n",
    "        fout = func(Xs[pos_idx:pos_idx+1, :, :, :])[0, filter_idx, :, :]\n",
    "    else:\n",
    "        fout = func(Xs[pos_idx:pos_idx+1, :, :, :])[0, :, :, :].sum(axis=0)\n",
    "        \n",
    "    if not ('vmin' in imshowkws.keys()):\n",
    "        if func==filter_output_fn:\n",
    "            ax.imshow(fout, vmin=-7.6, vmax=7.6, **imshowkws)\n",
    "        else:\n",
    "            ax.imshow(fout, vmin=-1, vmax=1, **imshowkws)\n",
    "    else:\n",
    "        ax.imshow(fout, **imshowkws)\n",
    "    plt.setp(ax, frame_on=False, xticklabels=[], yticklabels=[], xlabel='Filter response')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "sns.palplot(sns.diverging_palette(20, 240, n=11, s=99, as_cmap=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(33, 5, figsize=(24, 136), squeeze=False)\n",
    "\n",
    "pos_idx = 50 #30\n",
    "\n",
    "for i in np.arange(32):\n",
    "    \n",
    "    ax = axes[i+1, 0]\n",
    "    if i%2==0:\n",
    "        show_filter_output(pos_idx, filter_idx=i/2, ax=ax, func=pooled_foutput_fn)\n",
    "        plt.setp(ax, xlabel='Filter response (post-pooling)')\n",
    "    else:\n",
    "        plt.setp(ax, frame_on=False, xticklabels=[], yticklabels=[])\n",
    "    \n",
    "    ax = axes[i+1, 1]\n",
    "    show_filter_output(pos_idx, filter_idx=i, ax=ax, func=scaled_foutput_fn)\n",
    "    plt.setp(ax, xlabel='Filter response (post-PReLu)')\n",
    "\n",
    "    \n",
    "    ax = axes[i+1, 2]\n",
    "    show_filter_output(pos_idx, filter_idx=i, ax=ax)\n",
    "    plt.setp(ax, xlabel='Filter response')\n",
    "\n",
    "\n",
    "    ax = axes[i+1, 3]\n",
    "    ax.imshow(filters[i, 0, :, :], vmin=-2.6, vmax=2.6, **imshowkws)\n",
    "    plt.setp(ax, frame_on=False, xticklabels=[], yticklabels=[], xlabel='Own filter')\n",
    "\n",
    "\n",
    "    ax = axes[i+1 , 4]\n",
    "    ax.imshow(filters[i, 1, :, :],vmin=-2.6, vmax=2.6, **imshowkws)\n",
    "    plt.setp(ax, frame_on=False, xticklabels=[], yticklabels=[], xlabel='Opp filter')\n",
    "\n",
    "\n",
    "# make separate figure\n",
    "ax = axes[0, 0]\n",
    "kws = {'interpolation':'nearest', \n",
    "       'cmap': hmkws['cmap'], \n",
    "#        'vmin': -46, 'vmax':-7\n",
    "      }\n",
    "show_filter_output(pos_idx, func=pooled_foutput_fn, ax=ax, imshowkws=kws)\n",
    "plt.setp(ax, xlabel='Filter response (sum, post-pooling)') #.format(kws['vmin'], kws['vmax']))\n",
    "\n",
    "\n",
    "ax = axes[0, 1]\n",
    "show_net_response(pos_idx, ax=ax)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "\n",
    "ax = axes[0, 2:]\n",
    "plt.setp(ax, frame_on=False, xticklabels=[], yticklabels=[])\n",
    "\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hick's Law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xx, yy, Ss, G, Np = loading.unpack_data(df)\n",
    "\n",
    "def countpieces(row):\n",
    "    bp = row['bp']\n",
    "    wp = row['wp']\n",
    "    p = np.array(list(bp+wp)).astype(int)\n",
    "    return p.sum()\n",
    "\n",
    "df['npieces'] = df.apply(countpieces, axis=1)\n",
    "\n",
    "# rewrite as vectorized func for pandas!\n",
    "for s in np.arange(40):\n",
    "    for i in np.arange(df.loc[df['subject']==s, 'npieces'].max()):\n",
    "        c = (df['npieces']==i)&(df['subject']==s)\n",
    "        df.loc[c, 'mc rt'] = df.loc[c, 'rt'] - df.loc[c, 'rt'].mean()\n",
    "        df.loc[c, 'mc rt'] = df.loc[c, 'mc rt'] / df.loc[c, 'mc rt'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = df.groupby('npieces')\n",
    "rt_hists = g['rt'].apply(lambda x: np.histogram(x, bins=1000)[0])\n",
    "rt_hists = rt_hists.map(entropy).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatterkws = {\n",
    "    'marker':'o', 'markersize':6, 'linestyle': 'None', 'alpha': .15\n",
    "}\n",
    "\n",
    "predictions = net.output_fn(Xx)\n",
    "\n",
    "numlegal = 36 - df['npieces'].values\n",
    "df['numlegal'] = numlegal\n",
    "numlegalent = numlegal * (1 / numlegal) * np.log(1 / numlegal)\n",
    "entropies = np.apply_along_axis(entropy, axis=1, arr=predictions)\n",
    "# entropies = entropies + numlegalent\n",
    "# rt_np_ent = df.pivot_table(index='numlegal', values='rt', aggfunc=entropy).values\n",
    "\n",
    "response_times = df['rt'].values\n",
    "\n",
    "v1 = (response_times < 60000)\n",
    "v2 = (numlegal < 32)\n",
    "valid =  v1 & v2\n",
    "valid = np.where(valid)[0]\n",
    "aggents = [entropies[np.where(numlegal==i)[0]] for i in np.arange(1, 37)]\n",
    "meanents = np.array([m.mean() for m in aggents])\n",
    "sements = 1.96*np.array([m.std() / np.sqrt(m.size) for m in aggents])\n",
    "print(\"Entropy vs log RT\\n\", linregress(entropies[np.where(v2)[0]], np.log(response_times[np.where(v2)[0]])), '\\n')\n",
    "print(\"Spearman R Numlegal vs Entropy\\n\", spearmanr(numlegal, entropies), '\\n')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12), squeeze=False)\n",
    "v = np.where(v1)[0]\n",
    "ax = axes[0, 0]\n",
    "\n",
    "ax.plot(entropies[v], response_times[v], **scatterkws)\n",
    "plt.setp(ax, xlabel='Prediction entropy', ylabel='Response time (ms)')\n",
    "\n",
    "ax = axes[0, 1]\n",
    "ax.plot(numlegal[v], response_times[v], **scatterkws)\n",
    "plt.setp(ax, xlabel='# Legal moves', ylabel='Response time (ms)')\n",
    "\n",
    "ax = axes[1, 0]\n",
    "ax.plot(numlegal, entropies, **scatterkws)\n",
    "ax.plot(np.arange(1, 37), meanents, linewidth=5, label='Mean')\n",
    "ax.fill_between(\n",
    "    np.arange(1, 37), y1=meanents+sements, y2=meanents-sements, \n",
    "    alpha=.5, color=colors[1]\n",
    ")\n",
    "ax.legend(loc=0)\n",
    "plt.setp(ax, xlabel='# Legal moves', ylabel='Prediction entropy')\n",
    "\n",
    "ax = axes[1, 1]\n",
    "ax.plot(meanents, rt_hists, marker='o', linestyle='none', markersize=10)\n",
    "plt.setp(ax, xlabel='Mean Entropy per # legal moves', ylabel='RT Entropy per # legal moves')\n",
    "# plt.setp(ax, frame_on=False, xticklabels=[], yticklabels=[])\n",
    "\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
