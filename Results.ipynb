{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d90a54e3-3a80-4670-b7f9-16344f67f08b"
    }
   },
   "source": [
    "# Results\n",
    "\n",
    "Testing networks and saving results\n",
    "\n",
    "TODO:\n",
    "- clean up repeated bits into functions\n",
    "- save csv files with results to share/use elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "bbbb3ea3-a3ad-467f-bf63-edecc84fae20"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import yaml\n",
    "from functools import reduce\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import lasagne\n",
    "import loading\n",
    "from training import *\n",
    "from network import *\n",
    "from architectures import *\n",
    "from scipy.stats import bayes_mvs, entropy, linregress, spearmanr\n",
    "\n",
    "# aliases\n",
    "L = lasagne.layers\n",
    "nl = lasagne.nonlinearities\n",
    "T = theano.tensor\n",
    "bmvs = bayes_mvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5c91b643-dfe7-4296-bf7f-189ff156605d"
    }
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "a6c5f1f9-5c8c-4fba-9aca-65e87ee14e77"
    }
   },
   "outputs": [],
   "source": [
    "headdir = os.path.expanduser('~/Google Drive/Bas Zahy Gianni - Games')\n",
    "paramsdir = os.path.join(headdir, 'Analysis/0_hvh/Params/nnets/temp')\n",
    "datadir = os.path.join(headdir, 'Data/model input')\n",
    "resultsdir = os.path.join(headdir, 'Analysis/0_hvh/Loglik/nnets')\n",
    "\n",
    "data = loading.default_loader(os.path.join(datadir, '1-4 (no computer).csv'))\n",
    "hvhdata = loading.default_loader(os.path.join(datadir, '0 (with groups).csv'))\n",
    "df = hvhdata[0]\n",
    "Xs = np.concatenate(hvhdata[2])\n",
    "ys = np.concatenate(hvhdata[3])\n",
    "Ss = np.concatenate(hvhdata[4])\n",
    "\n",
    "defmod = np.loadtxt(os.path.expanduser('~/Downloads/loglik_hvh_final.txt')).reshape([40, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9a3a2e64-2ff9-46e2-bbd8-befcce0fcd2f"
    }
   },
   "source": [
    "## Compile Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unfreeze(net):\n",
    "    for layer in L.get_all_layers(net.net):\n",
    "        for param in layer.params:\n",
    "            layer.params[param].add('trainable')\n",
    "    net.params = L.get_all_params(net.net, trainable=True)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def load_arch_specs(archname):\n",
    "    with open('arch_specs.yaml') as archfile:\n",
    "        arch = yaml.load(archfile)\n",
    "    return arch[archname]\n",
    "\n",
    "def load_pretrain_results(archname, archfunc, idx, net=None):\n",
    "    specs = load_arch_specs(archname)\n",
    "    arch = lambda input_var: archfunc(input_var, **specs['kwargs'])\n",
    "    fname = '{} {} split agg fit exp 1-4.npz'.format(archname, idx)\n",
    "    resdf = pd.DataFrame(index=np.arange(Xt.shape[0]), columns=[idx])\n",
    "\n",
    "    if net is None:\n",
    "        net = Network(arch)\n",
    "    \n",
    "    net.load_params(os.path.join(paramsdir, fname))\n",
    "    res = net.itemized_test_fn(Xt, yt)\n",
    "    resdf[idx] = res\n",
    "    return resdf, net\n",
    "\n",
    "def load_train_results(archname, archfunc, idx, test_idx, net=None):\n",
    "    specs = load_arch_specs(archname)\n",
    "    arch = lambda input_var: archfunc(input_var, **specs['kwargs'])\n",
    "    fname = '{} {} agg fit exp 1-4 {} tune fit exp 0.npz'.format(archname, idx, test_idx)\n",
    "    resdf = pd.DataFrame(index=np.arange(Xt.shape[0]), columns=[idx])\n",
    "    \n",
    "    if net is None:\n",
    "        net = Network(arch)\n",
    "    \n",
    "    net.load_params(os.path.join(paramsdir, fname))\n",
    "    \n",
    "    group_idx = (test_idx - 1) % 5\n",
    "    selection = df.loc[df['group']==(group_idx+1)].index.values\n",
    "    res = net.itemized_test_fn(Xt[selection, :, :, :], yt[selection])\n",
    "    resdf.loc[selection, idx] = res\n",
    "    return resdf, net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "archname = 'multiconvX_ws_large'\n",
    "archfunc = multiconvX_ws\n",
    "Xt, yt, _, _, _ = loading.unpack_data(df)\n",
    "\n",
    "pretrain_nets = []\n",
    "pretrain_results = []\n",
    "# net = Network()\n",
    "\n",
    "for idx in range(5):\n",
    "    resdf, net = load_pretrain_results(archname, archfunc, idx)\n",
    "    pretrain_nets.append(net)\n",
    "    pretrain_results.append(resdf)\n",
    "\n",
    "pretrain_results = pd.concat(pretrain_results, axis=1)\n",
    "pretrain_results.to_csv(os.path.join(resultsdir, 'pretrain {}.csv'.format(archname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_nets = []\n",
    "train_results = []\n",
    "for idx in range(5):\n",
    "    for test_idx in range(5):\n",
    "        resdf, net = load_train_results(archname, archfunc, idx, test_idx)\n",
    "        train_nets.append(net)\n",
    "        train_results.append(resdf)\n",
    "\n",
    "train_results = pd.concat(train_results, axis=1, join='inner').stack().unstack()\n",
    "train_results.to_csv(os.path.join(resultsdir, 'train {}.csv'.format(archname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_results['mean'] = train_results.mean(axis=1)\n",
    "bmvs(train_results.pivot_table(index=df['subject'], values='mean').values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_num_params(net):\n",
    "    l = L.get_all_param_values(net)\n",
    "    return np.array([p.size for p in l])\n",
    "\n",
    "get_num_params(net.net).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "plt.rc('text', usetex=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatterkws = {\n",
    "    'linestyle': 'none', \n",
    "    'marker': 'o', 'markerfacecolor': (.2, .2, .2), 'markeredgecolor': 'black', \n",
    "    'alpha': .3\n",
    "}\n",
    "\n",
    "histkws = {\n",
    "    'edgecolor': 'white'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hicks_entropy(pred):\n",
    "    H = pred * np.log2(1 / (pred + 1))\n",
    "    return H.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, S, G, Np = loading.unpack_data(df)\n",
    "df['mean corrected rt'] = 0\n",
    "for subject in df['subject'].unique():\n",
    "    fil = df['subject'] == subject\n",
    "    df.loc[fil, 'mean corrected rt'] = df.loc[fil, 'rt'] - df.loc[fil, 'rt'].mean()\n",
    "\n",
    "rt = df['mean corrected rt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute mean entropy for each test group\n",
    "E = []\n",
    "for split_idx in range(25):\n",
    "    N = train_nets[split_idx]\n",
    "    locs = np.where(G==(split_idx//5))[0]\n",
    "    L = N.output_fn(X[locs, :, :, :])\n",
    "    E.append(hicks_entropy(L))\n",
    "\n",
    "for g in range(5):\n",
    "    df.loc[df['group']==(g+1), 'entropy'] = np.array(E[g*5:(g+1)*5]).T.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "x = df['entropy']\n",
    "y = np.log(df['rt']/1000)\n",
    "axes.plot(x, y, **scatterkws)\n",
    "lr = linregress(x, y)\n",
    "pval = lr.pvalue if lr.pvalue >= .001 else .001\n",
    "axes.text(.05, .05, r\"r = {:.2f}, p $<$ {:.3f}\".format(lr.rvalue, pval), transform=axes.transAxes, fontsize=14)\n",
    "plt.setp(axes, xlabel=r\"$\\textrm{Entropy}$\", ylabel=r'$\\log{\\textrm{Response time (s)}}$', ylim=[-5, 5])\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hick's law holds (ish)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gendata = pd.read_csv(\n",
    "    os.path.join(headdir, 'Data/1_gen/Clean/_summaries/all_evals_model_input.csv'),\n",
    "    names=['subject', 'color', 'bp', 'wp', 'zet', 'rt', 'val']\n",
    ")\n",
    "gendata['group'] = -1\n",
    "\n",
    "X, y, S, G, Np = loading.unpack_data(gendata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = train_nets[0]\n",
    "logistic = lambda x: 1 / (1 + np.exp(-x))\n",
    "zscore = lambda x: (x - x.mean()) / (x.std() / np.sqrt(x.shape[0]))\n",
    "Vr = N.value_fn(X)\n",
    "V = Vr.sum(axis=1)\n",
    "Vl = 7*logistic(zscore(V))\n",
    "\n",
    "V2 = np.zeros_like(V)\n",
    "yz = np.zeros_like(y)\n",
    "for subject in range(S.max()):\n",
    "    V2[S==subject] = zscore(V[S==subject])\n",
    "    yz[S==subject] = zscore(y[S==subject])\n",
    "    \n",
    "V2l = 7*logistic(V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Vr = N.value_fn(X) - N.value_fn(X[:, ::-1, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(V, **histkws)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(V2, **histkws) #, bins=np.arange(0, 8, .5), **histkws)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(V, gendata['val'], **scatterkws)\n",
    "print(linregress(V2, gendata['val']))\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(gendata['val'], gendata['zet'], **scatterkws)\n",
    "print(linregress(gendata['zet'], gendata['val']))\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(zscore(V), yz, **scatterkws)\n",
    "print(linregress(zscore(V), yz))\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gendata['valhat'] = 6*logistic(V2) + 1\n",
    "gendata['valhat'] = gendata['valhat'].map(int)\n",
    "gendata['position'] = gendata['bp'] + gendata['wp']\n",
    "gp = gendata.pivot_table(index='position', columns='zet', values='group', aggfunc=len, fill_value=0)\n",
    "gvp = gendata.pivot_table(index='position', values='valhat')\n",
    "gp['valhat'] = gvp.values\n",
    "gp['valsum'] = gp[list(np.arange(1, 8, 1))].values.argmax(axis=1) + 1\n",
    "gp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linregress(gp['valhat'], gp['valsum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old stuff (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c971ad68-0a00-4398-9d89-5f28953b5f40"
    }
   },
   "outputs": [],
   "source": [
    "# num_filters = [4, 8, 16, 32, 64, 128]\n",
    "# archnames = [\"arch{}\".format(n) for n in num_filters]\n",
    "# columns = pd.MultiIndex.from_product([archnames, np.arange(5).astype(str)])\n",
    "# tune_tidy = pd.DataFrame(index=df.index) #, columns=columns)\n",
    "# tune_tidy['subject'] = df['subject']\n",
    "# tune_tidy['group'] = df['group'] - 1\n",
    "\n",
    "# pretrain_tidy = pd.DataFrame(index=df.index) #, columns=columns)\n",
    "# pretrain_tidy['subject'] = df['subject']\n",
    "# pretrain_tidy['group'] = df['group'] - 1\n",
    "\n",
    "\n",
    "# subtune_tidy = pd.DataFrame(index=df.index) #, columns=columns)\n",
    "# subtune_tidy['subject'] = df['subject']\n",
    "# subtune_tidy['group'] = df['group'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c4bd7cf7-0d3d-4a34-b9bc-d8f32d3e7ebf"
    }
   },
   "outputs": [],
   "source": [
    "for nfil in num_filters:\n",
    "    for prefit_idx in range(5):\n",
    "        for test_split in range(5):\n",
    "            # create network container with nfil filters architecture\n",
    "            if nfil == 32:\n",
    "                fname = '{} agg fit exp 1-4 {} tune fit exp 0.npz'.format(prefit_idx, test_split)\n",
    "            else:\n",
    "                fname = 'arch{} {} agg fit exp 1-4 {} tune fit exp 0.npz'.format(nfil, prefit_idx, test_split)\n",
    "            \n",
    "            arch = lambda input_var: archX(input_var, num_filters=nfil)\n",
    "            net = Network(arch)\n",
    "            net.load_params(os.path.join(paramsdir, fname))\n",
    "            \n",
    "            group_idx = (test_split-1)%5 # fuck this up if you want to see overfitting\n",
    "            test_data = df.loc[df['group']==(group_idx+1)]\n",
    "            Xt, yt, St, Gt, Npt = loading.unpack_data(test_data)\n",
    "            res = net.itemized_test_fn(Xt, yt)\n",
    "            l1 = 'arch{}'.format(nfil)\n",
    "            l2 = str(prefit_idx)\n",
    "            tune_tidy.loc[tune_tidy['group']==group_idx, (l1, l2)] = res\n",
    "\n",
    "        if nfil == 32:\n",
    "            fname = '{} split agg fit exp 1-4.npz'.format(prefit_idx)\n",
    "        else:\n",
    "            fname = 'arch{} {} split agg fit exp 1-4.npz'.format(nfil, prefit_idx)\n",
    "            \n",
    "        arch = lambda input_var: archX(input_var, num_filters=nfil)\n",
    "        net = Network(arch)\n",
    "        net.load_params(os.path.join(paramsdir, fname))\n",
    "        Xt, yt, St, Gt, Npt = loading.unpack_data(df)\n",
    "        res = net.itemized_test_fn(Xt, yt)\n",
    "        l1 = 'arch{}'.format(nfil)\n",
    "        l2 = str(prefit_idx)\n",
    "        pretrain_tidy[(l1, l2)] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "433a94e9-2ec5-4884-962e-4e72fb4167a1"
    }
   },
   "outputs": [],
   "source": [
    "def pivtidy(tidydf):\n",
    "    t = tidydf[archnames].astype(float)\n",
    "    t = t.mean(axis=1, level=0)\n",
    "    t['subject'] = tidydf['subject']\n",
    "    t['group'] = tidydf['group']\n",
    "    tpiv = t.pivot_table(index='subject', values=archnames)\n",
    "    return tpiv\n",
    "\n",
    "pretrain_piv = pivtidy(pretrain_tidy)\n",
    "tune_piv = pivtidy(tune_tidy)\n",
    "\n",
    "print('pretrain agg\\n')\n",
    "for arc in archnames:\n",
    "    print(arc, \"\\n\", bmvs(pretrain_tidy[arc].values), \"\\n\")\n",
    "\n",
    "print('\\npretrain sub\\n')\n",
    "for arc in archnames:\n",
    "    print(arc, \"\\n\", bmvs(pretrain_piv[arc].values), \"\\n\")\n",
    "\n",
    "print('\\ntune agg\\n')\n",
    "for arc in archnames:\n",
    "    print(arc, \"\\n\", bmvs(tune_tidy[arc].values), \"\\n\")\n",
    "\n",
    "print('\\ntune sub\\n')\n",
    "for arc in archnames:\n",
    "    print(arc, \"\\n\", bmvs(tune_piv[arc].values), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "89a6f233-3de6-4174-894c-84b402d7643b"
    }
   },
   "outputs": [],
   "source": [
    "# ------ OLD -------\n",
    "# arch = lambda input_var: multiconvX(input_var, subnet_specs=[(4, (1, 4)), (4, (4, 1)), (4, (4, 4))])\n",
    "# num_filters = [4, 8, 16, 32, 64, 128]\n",
    "# archnames = [\"arch{}\".format(n) for n in num_filters]\n",
    "# columns = pd.MultiIndex.from_product([archnames, np.arange(5).astype(str)])\n",
    "# tune_tidy = pd.DataFrame(index=df.index) #, columns=columns)\n",
    "# tune_tidy['subject'] = df['subject']\n",
    "# tune_tidy['group'] = df['group'] - 1\n",
    "\n",
    "# pretrain_tidy = pd.DataFrame(index=df.index) #, columns=columns)\n",
    "# pretrain_tidy['subject'] = df['subject']\n",
    "# pretrain_tidy['group'] = df['group'] - 1\n",
    "\n",
    "\n",
    "# subtune_tidy = pd.DataFrame(index=df.index) #, columns=columns)\n",
    "# subtune_tidy['subject'] = df['subject']\n",
    "# subtune_tidy['group'] = df['group'] - 1\n",
    "\n",
    "archspecs = load_arch_specs(archname)\n",
    "archs = dict()\n",
    "archs[archname] = lambda input_var: multiconvX_ws(input_var, **archspecs['kwargs'])\n",
    "arch = archs[archname]\n",
    "\n",
    "net = Network(arch)\n",
    "\n",
    "\n",
    "for prefit_idx in range(5):\n",
    "    for test_split in range(5):\n",
    "        \n",
    "        # create network container with nfil filters architecture\n",
    "        fname = '{} {} agg fit exp 1-4 {} tune fit exp 0.npz'.format(archname, prefit_idx, test_split)\n",
    "        net.load_params(os.path.join(paramsdir, fname))\n",
    "\n",
    "        group_idx = (test_split-1)%5 # fuck this up if you want to see overfitting\n",
    "        test_data = df.loc[df['group']==(group_idx+1)]\n",
    "        Xt, yt, St, Gt, Npt = loading.unpack_data(test_data)\n",
    "        res = net.itemized_test_fn(Xt, yt)\n",
    "        l1 = 'multiconvX'\n",
    "        l2 = str(prefit_idx)\n",
    "        tune_tidy.loc[tune_tidy['group']==group_idx, (l1, l2)] = res\n",
    "        \n",
    "        for subject in range(40):\n",
    "            fname = '{} {} agg fit exp 1-4 {} subject {} tune fit exp 0.npz'.format(\n",
    "                archname, prefit_idx, subject, test_split\n",
    "            )\n",
    "            net.load_params(os.path.join(paramsdir, fname))\n",
    "            c = (subtune_tidy['group']==group_idx) & (subtune_tidy['subject']==subject)\n",
    "            test_data = df.loc[(df['group']==(group_idx+1))&(df['subject']==subject)]\n",
    "            Xt, yt, St, Gt, Npt = loading.unpack_data(test_data)\n",
    "            subtune_tidy.loc[c, (l1, l2)] = net.itemized_test_fn(Xt, yt)\n",
    "\n",
    "    fname = '{} {} split agg fit exp 1-4.npz'.format(archname, prefit_idx)\n",
    "    net.load_params(os.path.join(paramsdir, fname))\n",
    "    Xt, yt, St, Gt, Npt = loading.unpack_data(df)\n",
    "    res = net.itemized_test_fn(Xt, yt)\n",
    "    l1 = 'multiconvX'\n",
    "    l2 = str(prefit_idx)\n",
    "    pretrain_tidy[(l1, l2)] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "794c6adb-7357-4948-af68-6377fccbfd85"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pt = tune_tidy[['subject', 'group']].copy()\n",
    "pt.columns = pt.columns.get_level_values(0)\n",
    "pt['multiconvX'] = tune_tidy['multiconvX'].mean(axis=1)\n",
    "ptpiv = pt.pivot_table(index='subject', values='multiconvX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bmvs(ptpiv.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pt2 = tune_tidy[['subject', 'group']].copy()\n",
    "pt2.columns = pt2.columns.get_level_values(0)\n",
    "pt2[list(range(5))] = tune_tidy['multiconvX']\n",
    "pt2piv = pt2.pivot_table(index='subject', values=list(range(5)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
