{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d90a54e3-3a80-4670-b7f9-16344f67f08b"
    }
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "bbbb3ea3-a3ad-467f-bf63-edecc84fae20"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import bayes_mvs, entropy, linregress, spearmanr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import theano\n",
    "import lasagne\n",
    "\n",
    "import loading\n",
    "from training import *\n",
    "from network import *\n",
    "import architectures as arches\n",
    "from results import *\n",
    "\n",
    "# aliases\n",
    "L = lasagne.layers\n",
    "T = theano.tensor\n",
    "\n",
    "# styles\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "plt.rc('text', usetex=True)\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5c91b643-dfe7-4296-bf7f-189ff156605d"
    }
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "a6c5f1f9-5c8c-4fba-9aca-65e87ee14e77"
    }
   },
   "outputs": [],
   "source": [
    "# you do the below a LOT, consider making it an import\n",
    "headdir = os.path.expanduser('~/Google Drive/Bas Zahy Gianni - Games')\n",
    "paramsdir_ = os.path.join(headdir, 'Analysis/0_hvh/Params/nnets/')\n",
    "datadir = os.path.join(headdir, 'Data/model input')\n",
    "resultsdir = os.path.join(headdir, 'Analysis/0_hvh/Loglik/nnets')\n",
    "\n",
    "data = loading.default_loader(os.path.join(datadir, '1-4 (no computer).csv'))\n",
    "fake_data = loading.default_loader(os.path.join(datadir, 'fake news (with groups).csv'))\n",
    "hvhdata = loading.default_loader(os.path.join(datadir, '0 (with groups).csv'))\n",
    "df = hvhdata[0]\n",
    "Xs = np.concatenate(hvhdata[2])\n",
    "ys = np.concatenate(hvhdata[3])\n",
    "Ss = np.concatenate(hvhdata[4])\n",
    "\n",
    "defmod = np.loadtxt(os.path.expanduser('~/Downloads/loglik_hvh_final.txt')).reshape([40, 5])\n",
    "\n",
    "with open('arch_specs.yaml') as archfile:\n",
    "    arch_dict = yaml.load(archfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9a3a2e64-2ff9-46e2-bbd8-befcce0fcd2f"
    }
   },
   "source": [
    "## Compile Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xt, yt, _, _, _ = loading.unpack_data(df)\n",
    "\n",
    "# move below into results.py at some point\n",
    "\n",
    "PTR = {}\n",
    "TR = {}\n",
    "param_counts = {}\n",
    "\n",
    "for archname in arch_dict.keys():\n",
    "    arch_dir = archname[:-1]\n",
    "\n",
    "    if arch_dir not in os.listdir(paramsdir_):\n",
    "        print(\"{} not started\".format(archname[:-1]))\n",
    "        continue\n",
    "    \n",
    "    files = os.listdir(os.path.join(paramsdir_, arch_dir))\n",
    "    if not any(archname.replace('_', ' ') in f for f in files):\n",
    "        print(\"{} not started\".format(archname))\n",
    "        continue\n",
    "        \n",
    "    if archname in ['deep_c1']:\n",
    "        print(archname, 'not found')\n",
    "        continue\n",
    "\n",
    "    print('compiling', archname)\n",
    "    arch = arch_dict[archname]\n",
    "    af = getattr(arches, arch['type'])\n",
    "    arch_func = lambda input_var: af(input_var, **arch['kwargs'])\n",
    "    net = Network(arch_func)\n",
    "    \n",
    "    param_counts[archname] = L.count_params(net.net)\n",
    "    pretrain_R, pretrain_P, tune_R, tune_P = compute_net_results(net, archname, (Xt, yt), df)\n",
    "\n",
    "    PTR[archname] = pretrain_R\n",
    "    TR[archname] = tune_R\n",
    "    \n",
    "    pretrain_R.to_csv(os.path.join(resultsdir, 'pretrain {}.csv'.format(archname)))\n",
    "    tune_R.to_csv(os.path.join(resultsdir, 'train {}.csv'.format(archname)))\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# aggregate results into a single df\n",
    "\n",
    "pc_series = pd.Series(param_counts)\n",
    "pc_series.to_csv(os.path.join(resultsdir, 'params per net.csv'))\n",
    "F = pd.DataFrame(index=np.arange(len(pc_series.index)), columns=['net name'])\n",
    "F['net name'] = pc_series.index\n",
    "F['num params'] = pc_series.values\n",
    "\n",
    "for k, v in PTR.items():\n",
    "    idx = F['net name'] == k\n",
    "    v['subject'] = Ss\n",
    "    v['mean'] = v.mean()\n",
    "    F.loc[idx, 'pretrained'] = v['mean'].mean()\n",
    "    pvt = v.pivot_table(index='subject', values='mean', aggfunc=np.mean).mean().values\n",
    "    F.loc[idx, 'pretrained subject'] = pvt\n",
    "        \n",
    "for k, v in TR.items():\n",
    "    idx = F['net name'] == k\n",
    "    v['subject'] = Ss\n",
    "    v['mean'] = v.mean()\n",
    "    F.loc[idx, 'tuned'] = v['mean'].mean()\n",
    "    pvt = v.pivot_table(index='subject', values='mean', aggfunc=np.mean).mean().values\n",
    "    F.loc[idx, 'tuned subject'] = pvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F['tuning improvement'] = -(F['tuned'] - F['pretrained'])\n",
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(20, 3.5))\n",
    "\n",
    "for net_name in F['net name']:\n",
    "    if 'deep' in net_name:\n",
    "        color = 'green'\n",
    "    elif 'regular' in net_name:\n",
    "        color = 'red'\n",
    "    else:\n",
    "        color = 'purple'\n",
    "        \n",
    "    idx = F['net name'] == net_name\n",
    "    x = F.loc[idx, 'num params'].values[0]\n",
    "    y = F.loc[idx, 'tuned subject'].values[0]\n",
    "    axes.plot(x, y, linestyle='none', marker='o', color=color)\n",
    "\n",
    "plt.setp(axes, xlabel='# parameters', ylabel='NLL', xlim=[0, 75000])\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.to_csv(os.path.join(resultsdir, 'num params with nlls.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FD = fake_data[0]\n",
    "HH = hvhdata[0]\n",
    "AD = data[0]\n",
    "\n",
    "FD['position'] = FD['bp'] + FD['wp']\n",
    "HH['position'] = HH['bp'] + FD['wp']\n",
    "AD['position'] = AD['bp'] + AD['wp']\n",
    "\n",
    "FDpiv = FD.pivot_table(index='position', values='zet', aggfunc=entropy_zets)\n",
    "HHpiv = HH.pivot_table(index='position', values='zet', aggfunc=entropy_zets)\n",
    "ADpiv = AD.pivot_table(index='position', values='zet', aggfunc=entropy_zets)\n",
    "\n",
    "for piv in [FDpiv, ADpiv, HHpiv]:\n",
    "    print(piv.loc[piv.values[:, 0] > 0].mean())\n",
    "    print(len(piv), piv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archname = 'h4'\n",
    "Xt, yt, _, _, _ = loading.unpack_data(fake_data[0])\n",
    "\n",
    "fake_results = []\n",
    "fake_outputs = []\n",
    "\n",
    "specs = arch_dict[archname]\n",
    "af = getattr(arches, arch_dict[archname]['type'])\n",
    "arch_func = lambda input_var: af(input_var, **specs['kwargs'])\n",
    "net = Network(arch_func)\n",
    "\n",
    "for idx in range(5):\n",
    "    fname = '{} {} split agg fit exp 1-4.npz'.format(archname, idx)\n",
    "    paramsdir = os.path.join(paramsdir_, archname[:-1])\n",
    "\n",
    "    net.load_params(os.path.join(paramsdir, fname))\n",
    "    nlls = net.itemized_test_fn(Xt, yt)\n",
    "    predictions = net.output_fn(Xt)\n",
    "\n",
    "    fake_results.append(nlls)\n",
    "    fake_outputs.append(predictions)\n",
    "\n",
    "fake_results_df = pd.DataFrame(fake_results).T\n",
    "\n",
    "fake_results_df.pivot_table(index=fake_data[0]['subject']).mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['np'] = df.apply(count_pieces, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chancenll = lambda x: -np.log(1/(36-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['chancenll'] = chancenll(df['np'].values)\n",
    "df['m'] = -(train_results.mean(axis=1).values - df['chancenll'])\n",
    "np_v_m = df.pivot_table(index='np', values='m')\n",
    "np_v_m.to_csv(os.path.join(resultsdir, 'num_pieces_vs_nll.csv'), header=False)\n",
    "plt.plot(np_v_m)\n",
    "\n",
    "plt.setp(plt.gca(), xlabel='N Pieces', ylabel='NLL relative to chance', ylim=[-.5, 2])\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(df.pivot_table(index='np', values='chancenll'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Response Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatterkws = {\n",
    "    'linestyle': 'none', \n",
    "    'marker': 'o', 'markerfacecolor': (.2, .2, .2), 'markeredgecolor': 'black', \n",
    "    'alpha': .3\n",
    "}\n",
    "\n",
    "histkws = {\n",
    "    'edgecolor': 'white'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hicks_entropy(pred):\n",
    "    H = pred * np.log2(1 / (pred + 1))\n",
    "    return H.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, S, G, Np = loading.unpack_data(df)\n",
    "df['mean corrected rt'] = 0\n",
    "for subject in df['subject'].unique():\n",
    "    fil = df['subject'] == subject\n",
    "    df.loc[fil, 'mean corrected rt'] = df.loc[fil, 'rt'] - df.loc[fil, 'rt'].mean()\n",
    "\n",
    "rt = df['mean corrected rt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute mean entropy for each test group\n",
    "E = []\n",
    "for split_idx in range(25):\n",
    "    N = train_nets[split_idx]\n",
    "    locs = np.where(G==(split_idx//5))[0]\n",
    "    L = N.output_fn(X[locs, :, :, :])\n",
    "    E.append(hicks_entropy(L))\n",
    "\n",
    "for g in range(5):\n",
    "    df.loc[df['group']==(g+1), 'entropy'] = np.array(E[g*5:(g+1)*5]).T.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "x = df['entropy']\n",
    "y = np.log(df['rt']/1000)\n",
    "axes.plot(x, y, **scatterkws)\n",
    "lr = linregress(x, y)\n",
    "pval = lr.pvalue if lr.pvalue >= .001 else .001\n",
    "axes.text(.05, .05, r\"r = {:.2f}, p $<$ {:.3f}\".format(lr.rvalue, pval), transform=axes.transAxes, fontsize=14)\n",
    "plt.setp(axes, xlabel=r\"$\\textrm{Entropy}$\", ylabel=r'$\\log{\\textrm{Response time (s)}}$', ylim=[-5, 5])\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hick's law holds (ish)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gendata = pd.read_csv(\n",
    "    os.path.join(headdir, 'Data/1_gen/Clean/_summaries/all_evals_model_input.csv'),\n",
    "    names=['subject', 'color', 'bp', 'wp', 'zet', 'rt', 'val']\n",
    ")\n",
    "gendata['group'] = -1\n",
    "\n",
    "X, y, S, G, Np = loading.unpack_data(gendata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = train_nets[0]\n",
    "logistic = lambda x: 1 / (1 + np.exp(-x))\n",
    "zscore = lambda x: (x - x.mean()) / (x.std() / np.sqrt(x.shape[0]))\n",
    "Vr = N.value_fn(X)\n",
    "V = Vr.sum(axis=1)\n",
    "Vl = 7*logistic(zscore(V))\n",
    "\n",
    "V2 = np.zeros_like(V)\n",
    "yz = np.zeros_like(y)\n",
    "for subject in range(S.max()):\n",
    "    V2[S==subject] = zscore(V[S==subject])\n",
    "    yz[S==subject] = zscore(y[S==subject])\n",
    "    \n",
    "V2l = 7*logistic(V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Vr = N.value_fn(X) - N.value_fn(X[:, ::-1, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(V, **histkws)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(V2, **histkws) #, bins=np.arange(0, 8, .5), **histkws)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(V, gendata['val'], **scatterkws)\n",
    "print(linregress(V2, gendata['val']))\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(gendata['val'], gendata['zet'], **scatterkws)\n",
    "print(linregress(gendata['zet'], gendata['val']))\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(zscore(V), yz, **scatterkws)\n",
    "print(linregress(zscore(V), yz))\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gendata['valhat'] = 6*logistic(V2) + 1\n",
    "gendata['valhat'] = gendata['valhat'].map(int)\n",
    "gendata['position'] = gendata['bp'] + gendata['wp']\n",
    "gp = gendata.pivot_table(index='position', columns='zet', values='group', aggfunc=len, fill_value=0)\n",
    "gvp = gendata.pivot_table(index='position', values='valhat')\n",
    "gp['valhat'] = gvp.values\n",
    "gp['valsum'] = gp[list(np.arange(1, 8, 1))].values.argmax(axis=1) + 1\n",
    "gp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linregress(gp['valhat'], gp['valsum'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
